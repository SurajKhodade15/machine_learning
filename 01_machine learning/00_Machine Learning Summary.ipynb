{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d514d0d",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9f7ea5",
   "metadata": {},
   "source": [
    "\n",
    "## Machine Learning Introduction \n",
    "\n",
    "Machine learning is a subset of artificial intelligence (AI) focused on designing algorithms that enable computers to learn patterns and make decisions from data, without being directly programmed for every possible scenario. Unlike traditional programming (where explicit rules are coded), machine learning algorithms develop their own logic based on examples and feedback, improving performance as they are exposed to more data.\n",
    "\n",
    "**Types of Machine Learning**\n",
    "\n",
    "| Type                  | Description                                                                                   | Examples                        |\n",
    "|-----------------------|-----------------------------------------------------------------------------------------------|----------------------------------|\n",
    "| Supervised Learning   | Learns from labeled data to predict outcomes.                                                 | Email spam detection, regression |\n",
    "| Unsupervised Learning | Finds patterns and groupings in unlabeled data.                                               | Customer segmentation, clustering|\n",
    "| Reinforcement Learning| Learns by trial and error, receiving rewards/penalties from its environment.                  | Game playing, robotics           |\n",
    "\n",
    "**Key Concepts**\n",
    "\n",
    "- **Data:** The dataset used for training, crucial for effective learning.\n",
    "- **Model:** The algorithm or mathematical structure that learns from data (e.g., neural network, decision tree).\n",
    "- **Training:** The process where the model 'learns' patterns and adjusts its internal settings for best predictions.\n",
    "\n",
    "**Common Interview Questions**\n",
    "\n",
    "| Question                                                          | Key Point/Short Answer                                                                    |\n",
    "|-------------------------------------------------------------------|-------------------------------------------------------------------------------------------|\n",
    "| What is machine learning?                                          | The field where computer systems learn from data without explicit programming.             |\n",
    "| Types of machine learning?                                         | Supervised, Unsupervised, Reinforcement learning (see above table).                       |\n",
    "| Difference from traditional programming?                           | ML creates logic from data; traditional uses explicit rules coded by programmers.          |\n",
    "| Real-world applications?                                           | Spam filtering, recommendations, facial/speech recognition, fraud detection, self-driving.|\n",
    "| What are features and labels?                                      | Features are input variables; labels are the target outcomes for prediction tasks.         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ced90c6",
   "metadata": {},
   "source": [
    "## Supervised Machine Learning\n",
    "\n",
    "Supervised machine learning is a type of machine learning where algorithms are trained using labeled data, meaning each input has a corresponding correct output. The model learns the relationship between features (inputs) and labels (outputs) by analyzing these pairs. Its objective is to predict accurate outcomes for new, unseen data by generalizing from the training data patterns.\n",
    "\n",
    "**Types of Supervised Learning Tasks**\n",
    "- **Classification:** Predicting categorical labels (e.g., spam vs. non-spam emails).\n",
    "- **Regression:** Predicting continuous numerical values (e.g., house price prediction).\n",
    "\n",
    "**Key Process Steps**\n",
    "- Train the model on labeled data (features and labels).\n",
    "- Predict outputs and compare them with actual labels.\n",
    "- Adjust model parameters to minimize errors.\n",
    "- Evaluate performance on test data.\n",
    "\n",
    "| **Aspect**         | **Description**                                         | **Example Tasks**                |\n",
    "|--------------------|--------------------------------------------------------|----------------------------------|\n",
    "| Input Data         | Labeled data with features and corresponding labels     | Emails + spam/not spam labels    |\n",
    "| Goal               | Learn mapping from inputs to outputs                   | Classification, regression       |\n",
    "| Common Tasks       | Classification and regression                          | Spam detection, price prediction |\n",
    "| Evaluation         | Measure accuracy or error on test data                 | Accuracy, RMSE                   |\n",
    "\n",
    "**Common Interview Questions**\n",
    "\n",
    "| **Question**                             | **Key Point/Short Answer**                                                                |\n",
    "|------------------------------------------|------------------------------------------------------------------------------------------|\n",
    "| What is supervised learning?             | Learning from labeled data to predict future outcomes.                                   |\n",
    "| Difference from unsupervised learning?   | Supervised uses labeled data; unsupervised finds patterns in unlabeled data.             |\n",
    "| Classification vs. regression?           | Classification predicts categories; regression predicts continuous values.               |\n",
    "| What is overfitting and how to prevent it?| Overfitting occurs when the model learns noise; prevented by cross-validation, regularization, pruning. |\n",
    "| Bias-variance tradeoff?                  | The balance between underfitting (high bias) and overfitting (high variance).            |\n",
    "| Evaluation metrics?                      | Accuracy, Precision, Recall, F1 Score for classification; RMSE, MAE for regression.      |\n",
    "| Examples of supervised algorithms?       | Linear regression, logistic regression, decision trees, random forest, SVM, k-NN, neural networks. |\n",
    "| What is cross-validation?                | A technique to assess generalization by splitting data into multiple train/test sets.    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c106a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "260f177f",
   "metadata": {},
   "source": [
    "Here is the previous content with sections that can be logically represented in **tabular format** converted into tables:\n",
    "\n",
    "---\n",
    "\n",
    "## 📘 **Theory: Simple Linear Regression**\n",
    "\n",
    "**Definition:**\n",
    "Simple Linear Regression is a supervised learning algorithm used to predict a **continuous** target variable $y$ based on a **single** independent variable $x$. It assumes a **linear** relationship between $x$ and $y$ and fits a straight line to the data.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Model Equation**\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x + \\epsilon\n",
    "$$\n",
    "\n",
    "| Symbol     | Meaning                                        |\n",
    "| ---------- | ---------------------------------------------- |\n",
    "| $y$        | Dependent (target) variable                    |\n",
    "| $x$        | Independent (feature) variable                 |\n",
    "| $\\beta_0$  | Intercept (value of $y$ when $x=0$)            |\n",
    "| $\\beta_1$  | Slope (change in $y$ for a unit change in $x$) |\n",
    "| $\\epsilon$ | Error term                                     |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **How the Model Learns**\n",
    "\n",
    "| Step | Description                                                        |\n",
    "| ---- | ------------------------------------------------------------------ |\n",
    "| 1    | Estimates $\\beta_0$ and $\\beta_1$ to minimize prediction errors    |\n",
    "| 2    | Uses **Mean Squared Error (MSE)** as the cost function             |\n",
    "| 3    | Applies **Ordinary Least Squares (OLS)** to find the best-fit line |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Assumptions of Linear Regression**\n",
    "\n",
    "| Assumption       | Description                                |\n",
    "| ---------------- | ------------------------------------------ |\n",
    "| Linearity        | Relationship between $x$ and $y$ is linear |\n",
    "| Independence     | Observations are independent               |\n",
    "| Homoscedasticity | Residuals have constant variance           |\n",
    "| Normality        | Residuals are normally distributed         |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                           | Disadvantages                                |\n",
    "| ------------------------------------ | -------------------------------------------- |\n",
    "| Easy to implement and interpret      | Assumes linearity; fails on non-linear data  |\n",
    "| Computationally efficient            | Sensitive to outliers                        |\n",
    "| Provides a baseline for other models | Poor performance if assumptions are violated |\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                          | Answer                                                                                                           |\n",
    "| --------------------------------- | ---------------------------------------------------------------------------------------------------------------- |\n",
    "| What is simple linear regression? | It’s an algorithm that models a linear relationship between one independent variable and one dependent variable. |\n",
    "| Give a real-world example of SLR. | Predicting house price based on its size.                                                                        |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                              | Answer                                                                                              |\n",
    "| ----------------------------------------------------- | --------------------------------------------------------------------------------------------------- |\n",
    "| What is the cost function used in linear regression?  | Mean Squared Error (MSE).                                                                           |\n",
    "| How are parameters $\\beta_0$ and $\\beta_1$ estimated? | Using the Ordinary Least Squares (OLS) method, which minimizes the sum of squared residuals.        |\n",
    "| What is $R^2$ in linear regression?                   | A metric that explains the proportion of variance in the dependent variable explained by the model. |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                                                            | Answer                                                                                                             |\n",
    "| ----------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------ |\n",
    "| Explain the assumptions of linear regression and what happens if they are violated. | Violations may lead to biased or inefficient estimates; e.g., heteroscedasticity affects standard errors.          |\n",
    "| How would you detect and handle outliers in linear regression?                      | Use residual plots, leverage scores, Cook’s distance; handle by removing or applying robust regression techniques. |\n",
    "| Why is gradient descent not commonly used for simple linear regression?             | Because OLS has an analytical solution, making it computationally simpler.                                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cba6f7",
   "metadata": {},
   "source": [
    "## Cost Functions in Machine Learning\n",
    "\n",
    "Cost functions (also known as loss or objective functions) quantify how well a machine learning model is performing by measuring the difference between predicted values and actual values. The ultimate goal of training a model is to minimize the cost function, leading to better accuracy.\n",
    "\n",
    "### Common Cost Functions and Their Use Cases\n",
    "\n",
    "| **Cost Function**            | **Type**          | **Formula (for single sample)**                                               | **Use Case / Notes**                                                                                            |\n",
    "|-----------------------------|-------------------|-----------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|\n",
    "| Mean Squared Error (MSE)     | Regression        | $$\\frac{1}{2} (y_\\text{pred} - y_\\text{true})^2$$                          | Most popular for regression; penalizes large errors heavily; differentiable and convex                        |\n",
    "| Mean Absolute Error (MAE)    | Regression        | $$|y_\\text{pred} - y_\\text{true}|$$                                        | Robust to outliers; less sensitive than MSE; not differentiable at zero                                        |\n",
    "| Root Mean Squared Error (RMSE) | Regression      | $$\\sqrt{\\frac{1}{m} \\sum (y_\\text{pred} - y_\\text{true})^2}$$             | Square root of MSE, measuring error in original units                                                          |\n",
    "| Mean Absolute Percentage Error (MAPE) | Regression | $$\\frac{1}{m} \\sum \\left| \\frac{y_\\text{true} - y_\\text{pred}}{y_\\text{true}} \\right|$$ | Measures prediction accuracy in percentage terms                                                               |\n",
    "| Huber Loss                   | Regression        | Piecewise: quadratic if error < δ, linear otherwise                         | Combines advantages of MSE and MAE; less sensitive to outliers                                                 |\n",
    "| Binary Cross-Entropy         | Binary Classification | $$-[y \\log(p) + (1 - y) \\log(1 - p)]$$                                    | Measures error between predicted probabilities and actual classes                                              |\n",
    "| Categorical Cross-Entropy    | Multi-class Classification | $$-\\sum_k y_k \\log(p_k)$$                                                  | Extends binary cross-entropy to multi-class problems                                                            |\n",
    "| Hinge Loss                   | Classification    | $$\\max(0, 1 - y \\cdot f(x))$$                                              | Used by support vector machines; tries to maximize margin between classes                                      |\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Regression Tasks:** MSE, MAE, RMSE, MAPE, and Huber Loss are common. MSE is the most widely used because it penalizes large errors, but MAE and Huber Loss are more robust to outliers.\n",
    "- **Classification Tasks:** Cross-Entropy Loss (binary or categorical) is prevalent because it works well with probabilistic outputs from classifiers. Hinge Loss is used with support vector machines.\n",
    "- The choice depends on the problem type, data distribution, robustness needs, and model framework.\n",
    "\n",
    "### Role in Training\n",
    "\n",
    "- The cost function outputs a scalar error value representing model performance.\n",
    "- Optimization algorithms minimize this cost by adjusting model parameters.\n",
    "- Well-chosen cost functions lead to faster convergence and better generalization.\n",
    "\n",
    "This overview of cost functions can be directly incorporated into your Jupyter notebook for study or reference.\n",
    "\n",
    "[1] https://www.analyticssteps.com/blogs/7-types-cost-functions-machine-learning\n",
    "[2] https://intellipaat.com/blog/cost-function-in-machine-learning/\n",
    "[3] https://www.alooba.com/skills/concepts/machine-learning-11/cost-functions/\n",
    "[4] https://www.analytixlabs.co.in/blog/cost-function-in-machine-learning/\n",
    "[5] https://wisdomplexus.com/blogs/cost-function-in-machine-learning-meaning-types-and-importance/\n",
    "[6] https://www.numberanalytics.com/blog/ultimate-guide-cost-function-machine-learning\n",
    "[7] https://www.geeksforgeeks.org/machine-learning/ml-cost-function-in-logistic-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab2d59e",
   "metadata": {},
   "source": [
    "## 📘 **Theory: Multiple Linear Regression**\n",
    "\n",
    "**Definition:**\n",
    "Multiple Linear Regression (MLR) is an extension of simple linear regression where the target variable $y$ depends on **two or more independent variables** $x_1, x_2, ..., x_n$.\n",
    "\n",
    "* It models the relationship between multiple predictors and a continuous outcome.\n",
    "* The model fits a hyperplane (instead of a line) in an n-dimensional feature space.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Model Equation**\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\epsilon\n",
    "$$\n",
    "\n",
    "| Symbol               | Meaning                                                     |\n",
    "| -------------------- | ----------------------------------------------------------- |\n",
    "| $y$                  | Dependent (target) variable                                 |\n",
    "| $x_1, x_2, ..., x_n$ | Independent (feature) variables                             |\n",
    "| $\\beta_0$            | Intercept (value of $y$ when all $x_i=0$)                   |\n",
    "| $\\beta_i$            | Coefficient representing the effect of feature $x_i$ on $y$ |\n",
    "| $\\epsilon$           | Error term capturing noise in the data                      |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **How the Model Learns**\n",
    "\n",
    "| Step | Description                                                                                    |\n",
    "| ---- | ---------------------------------------------------------------------------------------------- |\n",
    "| 1    | Estimate coefficients $\\beta_0, \\beta_1, ..., \\beta_n$ using **Ordinary Least Squares (OLS)**. |\n",
    "| 2    | Minimize the **Mean Squared Error (MSE)** cost function.                                       |\n",
    "| 3    | The fitted model predicts $y$ by summing contributions from all features.                      |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Assumptions of Multiple Linear Regression**\n",
    "\n",
    "| Assumption           | Description                                           |\n",
    "| -------------------- | ----------------------------------------------------- |\n",
    "| Linearity            | Relationship between predictors and target is linear. |\n",
    "| Independence         | Observations are independent.                         |\n",
    "| Homoscedasticity     | Residuals have constant variance.                     |\n",
    "| Normality            | Residuals are normally distributed.                   |\n",
    "| No Multicollinearity | Predictors are not highly correlated with each other. |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                                           | Disadvantages                                          |\n",
    "| ---------------------------------------------------- | ------------------------------------------------------ |\n",
    "| Models relationships with multiple factors           | Sensitive to multicollinearity                         |\n",
    "| Easy to interpret (coefficients show feature impact) | Assumes linearity, may not capture non-linear patterns |\n",
    "| Efficient and widely used                            | Outliers can distort the model                         |\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                            | Answer                                                                                                           |\n",
    "| ----------------------------------- | ---------------------------------------------------------------------------------------------------------------- |\n",
    "| What is multiple linear regression? | It’s a regression technique that predicts a continuous target variable using more than one independent variable. |\n",
    "| Give an example of MLR.             | Predicting house prices using size, location, and number of rooms.                                               |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                                  | Answer                                                                                   |\n",
    "| --------------------------------------------------------- | ---------------------------------------------------------------------------------------- |\n",
    "| What cost function is used in multiple linear regression? | Mean Squared Error (MSE).                                                                |\n",
    "| What is multicollinearity?                                | When independent variables are highly correlated, making coefficient estimates unstable. |\n",
    "| How can you detect multicollinearity?                     | Using Variance Inflation Factor (VIF) or correlation matrices.                           |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                                 | Answer                                                                                                                 |\n",
    "| -------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- |\n",
    "| How do you handle multicollinearity?                     | Remove correlated variables, apply dimensionality reduction (e.g., PCA), or use regularization (Ridge/Lasso).          |\n",
    "| What metrics evaluate the model’s performance?           | $R^2$, Adjusted $R^2$, RMSE, MAE.                                                                                      |\n",
    "| What is the difference between $R^2$ and Adjusted $R^2$? | Adjusted $R^2$ penalizes the addition of irrelevant variables, giving a more reliable measure for multiple predictors. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8b2a0b",
   "metadata": {},
   "source": [
    "## 📘 **Theory: Performance Metrics in Machine Learning**\n",
    "\n",
    "**Definition:**\n",
    "Performance metrics are quantitative measures used to evaluate how well a machine learning model performs on unseen data.\n",
    "\n",
    "* The choice of metric depends on the type of problem: **Regression** or **Classification**.\n",
    "* Proper evaluation ensures the model generalizes well and is not overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Performance Metrics for Regression**\n",
    "\n",
    "| Metric                             | Formula                                   | Range          | Interpretation                                                  |               |                                      |\n",
    "| ---------------------------------- | ----------------------------------------- | -------------- | --------------------------------------------------------------- | ------------- | ------------------------------------ |\n",
    "| **Mean Squared Error (MSE)**       | $\\frac{1}{n} \\sum (\\hat{y} - y)^2$        | $[0, \\infty)$  | Lower is better; penalizes large errors heavily.                |               |                                      |\n",
    "| **Root Mean Squared Error (RMSE)** | $\\sqrt{\\frac{1}{n} \\sum (\\hat{y} - y)^2}$ | $[0, \\infty)$  | Same as MSE but in original units of $y$.                       |               |                                      |\n",
    "| **Mean Absolute Error (MAE)**      | (\\frac{1}{n} \\sum                         | \\hat{y} - y    | )                                                               | $[0, \\infty)$ | Less sensitive to outliers than MSE. |\n",
    "| **R-Squared ($R^2$)**              | $1 - \\frac{SS_{res}}{SS_{tot}}$           | $(-\\infty, 1]$ | Proportion of variance explained; closer to 1 is better.        |               |                                      |\n",
    "| **Adjusted $R^2$**                 | $1 - \\frac{(1 - R^2)(n - 1)}{n - p - 1}$  | $(-\\infty, 1]$ | Adjusts $R^2$ for number of predictors to avoid overestimation. |               |                                      |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Performance Metrics for Classification**\n",
    "\n",
    "| Metric                   | Formula                                                             | Range         | Interpretation                                                 |\n",
    "| ------------------------ | ------------------------------------------------------------------- | ------------- | -------------------------------------------------------------- |\n",
    "| **Accuracy**             | $\\frac{TP + TN}{TP + TN + FP + FN}$                                 | $[0, 1]$      | Percentage of correctly classified instances.                  |\n",
    "| **Precision**            | $\\frac{TP}{TP + FP}$                                                | $[0, 1]$      | Of predicted positives, how many are correct.                  |\n",
    "| **Recall (Sensitivity)** | $\\frac{TP}{TP + FN}$                                                | $[0, 1]$      | Of actual positives, how many are correctly identified.        |\n",
    "| **F1-Score**             | $2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}$         | $[0, 1]$      | Harmonic mean of precision and recall.                         |\n",
    "| **Specificity**          | $\\frac{TN}{TN + FP}$                                                | $[0, 1]$      | Ability to correctly identify negatives.                       |\n",
    "| **ROC-AUC**              | Area under ROC curve                                                | $[0, 1]$      | Higher values indicate better discrimination between classes.  |\n",
    "| **Log Loss**             | $-\\frac{1}{n} \\sum [ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) ]$ | $[0, \\infty)$ | Measures accuracy of probability predictions; lower is better. |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Confusion Matrix (for Classification)**\n",
    "\n",
    "|                     | Predicted Positive  | Predicted Negative  |\n",
    "| ------------------- | ------------------- | ------------------- |\n",
    "| **Actual Positive** | True Positive (TP)  | False Negative (FN) |\n",
    "| **Actual Negative** | False Positive (FP) | True Negative (TN)  |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Special Metrics (for Imbalanced Data)**\n",
    "\n",
    "* **Precision-Recall Curve:** Focuses on performance with imbalanced classes.\n",
    "* **Fβ-Score:** Weighted F-score giving more importance to either precision or recall.\n",
    "* **Matthews Correlation Coefficient (MCC):** Balanced metric even for skewed datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                                          | Answer                                                                |\n",
    "| ------------------------------------------------- | --------------------------------------------------------------------- |\n",
    "| What metric do you use for regression?            | Common metrics: MSE, RMSE, MAE, $R^2$.                                |\n",
    "| What metric do you use for binary classification? | Accuracy, Precision, Recall, F1-Score, ROC-AUC.                       |\n",
    "| What is a confusion matrix?                       | A table showing correct and incorrect predictions for classification. |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                      | Answer                                                                                    |\n",
    "| --------------------------------------------- | ----------------------------------------------------------------------------------------- |\n",
    "| Why is accuracy not always a good metric?     | In imbalanced datasets, accuracy can be misleading because it ignores class distribution. |\n",
    "| When would you prefer F1-score over accuracy? | When false positives and false negatives are equally important and data is imbalanced.    |\n",
    "| What does ROC-AUC measure?                    | The ability of a model to distinguish between classes at different thresholds.            |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                            | Answer                                                                                                                             |\n",
    "| --------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| What’s the difference between precision and recall? | Precision focuses on correctness of positive predictions, while recall measures coverage of actual positives.                      |\n",
    "| Why use adjusted $R^2$ instead of $R^2$?            | Adjusted $R^2$ accounts for number of predictors, preventing artificial inflation.                                                 |\n",
    "| How do you choose a metric for a business problem?  | Based on the cost of misclassification errors and project goals (e.g., recall in medical diagnosis, precision in fraud detection). |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2db5b",
   "metadata": {},
   "source": [
    "## 📘 **Theory: Overfitting and Underfitting in Machine Learning**\n",
    "\n",
    "**Definition:**\n",
    "Overfitting and underfitting are two common problems in model training that affect a model’s ability to generalize to unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Overfitting**\n",
    "\n",
    "| Aspect         | Description                                                                                                            |\n",
    "| -------------- | ---------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Definition** | When a model learns the training data too well, including noise and outliers, leading to poor performance on new data. |\n",
    "| **Cause**      | Model is too complex (too many parameters or features).                                                                |\n",
    "| **Symptoms**   | High training accuracy, low test accuracy.                                                                             |\n",
    "| **Example**    | A decision tree grown without pruning that memorizes training data.                                                    |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Underfitting**\n",
    "\n",
    "| Aspect         | Description                                                                                                                          |\n",
    "| -------------- | ------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| **Definition** | When a model is too simple to capture underlying patterns in the data, resulting in poor performance on both training and test data. |\n",
    "| **Cause**      | Model lacks complexity or is improperly trained.                                                                                     |\n",
    "| **Symptoms**   | Low training accuracy and low test accuracy.                                                                                         |\n",
    "| **Example**    | Using a linear model to fit non-linear data.                                                                                         |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Bias-Variance Trade-off**\n",
    "\n",
    "| Term         | Description                                                                          |\n",
    "| ------------ | ------------------------------------------------------------------------------------ |\n",
    "| **Bias**     | Error due to overly simplistic assumptions (underfitting).                           |\n",
    "| **Variance** | Error due to model sensitivity to small fluctuations in training data (overfitting). |\n",
    "| **Goal**     | Find a balance where both bias and variance are minimized.                           |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Techniques to Handle Overfitting**\n",
    "\n",
    "| Technique                 | Description                                             |\n",
    "| ------------------------- | ------------------------------------------------------- |\n",
    "| Cross-Validation          | Use validation data to tune model parameters.           |\n",
    "| Regularization (L1/L2)    | Penalize large coefficients to simplify the model.      |\n",
    "| Pruning (for trees)       | Limit depth or remove unnecessary branches.             |\n",
    "| Early Stopping            | Stop training before the model starts memorizing noise. |\n",
    "| Dropout (for neural nets) | Randomly drop neurons during training.                  |\n",
    "| Reduce Features           | Remove irrelevant or highly correlated features.        |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Techniques to Handle Underfitting**\n",
    "\n",
    "| Technique             | Description                                                  |\n",
    "| --------------------- | ------------------------------------------------------------ |\n",
    "| Add Features          | Include more informative predictors.                         |\n",
    "| Use Complex Models    | Choose models with higher capacity (e.g., ensemble methods). |\n",
    "| Reduce Regularization | Loosen constraints on parameters.                            |\n",
    "| Train Longer          | Allow model to learn more patterns from data.                |\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                       | Answer                                                                         |\n",
    "| ------------------------------ | ------------------------------------------------------------------------------ |\n",
    "| What is overfitting?           | It’s when a model memorizes training data and fails to generalize to new data. |\n",
    "| What is underfitting?          | It’s when a model is too simple and fails to capture data patterns.            |\n",
    "| How do you detect overfitting? | Compare training and validation accuracy; large gap indicates overfitting.     |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                    | Answer                                                                                                           |\n",
    "| ------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- |\n",
    "| Explain the bias-variance trade-off.        | High bias leads to underfitting; high variance leads to overfitting. The trade-off seeks optimal generalization. |\n",
    "| What techniques prevent overfitting?        | Cross-validation, regularization, pruning, dropout, early stopping.                                              |\n",
    "| How does regularization reduce overfitting? | It adds a penalty term to the cost function to prevent large coefficients and model complexity.                  |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                                                        | Answer                                                                                                                      |\n",
    "| ------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |\n",
    "| Why does adding more features sometimes lead to overfitting?                    | Because the model becomes more complex, capturing noise along with patterns.                                                |\n",
    "| How would you handle overfitting in a neural network?                           | Use dropout, early stopping, and data augmentation.                                                                         |\n",
    "| Can you explain a scenario where both overfitting and underfitting might occur? | When a model starts underfitting with insufficient training and then overfits as training continues without regularization. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f124dd69",
   "metadata": {},
   "source": [
    "## 📘 **Theory: Polynomial Linear Regression**\n",
    "\n",
    "**Definition:**\n",
    "Polynomial Linear Regression is an extension of simple and multiple linear regression where the relationship between the independent variable(s) and the dependent variable is modeled as an **nth-degree polynomial**.\n",
    "\n",
    "* It is still a **linear model** because coefficients are linear, but the features are transformed into polynomial terms.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Model Equation**\n",
    "\n",
    "For a single variable $x$ and polynomial degree $n$:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\dots + \\beta_n x^n + \\epsilon\n",
    "$$\n",
    "\n",
    "| Term                   | Meaning                             |\n",
    "| ---------------------- | ----------------------------------- |\n",
    "| $y$                    | Dependent (target) variable         |\n",
    "| $x$                    | Independent variable                |\n",
    "| $x^2, x^3, \\dots, x^n$ | Polynomial features (powers of $x$) |\n",
    "| $\\beta_i$              | Coefficients for each term          |\n",
    "| $\\epsilon$             | Error term                          |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **How It Works**\n",
    "\n",
    "| Step | Description                                                                   |\n",
    "| ---- | ----------------------------------------------------------------------------- |\n",
    "| 1    | Transform original feature(s) into polynomial features (e.g., $x^2, x^3$).    |\n",
    "| 2    | Apply linear regression on transformed features.                              |\n",
    "| 3    | Fit a curve (instead of a straight line) to capture non-linear relationships. |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                                       | Disadvantages                                   |\n",
    "| ------------------------------------------------ | ----------------------------------------------- |\n",
    "| Captures non-linear patterns easily              | High-degree polynomials may lead to overfitting |\n",
    "| Simple to implement with linear regression tools | Sensitive to outliers                           |\n",
    "| Provides flexibility in curve fitting            | Computationally expensive for high-degree terms |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Overfitting Risk**\n",
    "\n",
    "* Higher polynomial degrees can create a curve that fits the training data very closely (overfitting).\n",
    "* Regularization (Ridge, Lasso) can help control complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Use Cases**\n",
    "\n",
    "* Modeling growth curves (population, sales trends)\n",
    "* Capturing non-linear trends in time series\n",
    "* Engineering applications where relationships are polynomial\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                                       | Answer                                                                                                      |\n",
    "| ---------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |\n",
    "| What is polynomial regression?                 | It’s a regression technique where the model fits a polynomial equation to capture non-linear relationships. |\n",
    "| Is polynomial regression linear or non-linear? | It’s a linear model because it’s linear in terms of coefficients $\\beta$, though features are polynomial.   |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                                | Answer                                                                                                                       |\n",
    "| ------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |\n",
    "| How do you implement polynomial regression in practice? | Transform features using polynomial terms (e.g., via `PolynomialFeatures` in scikit-learn) and then apply linear regression. |\n",
    "| What happens when you increase polynomial degree?       | Model flexibility increases, but risk of overfitting also rises.                                                             |\n",
    "| How can you choose the right degree of the polynomial?  | Use cross-validation to determine the optimal complexity.                                                                    |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                                                                     | Answer                                                                                                                                  |\n",
    "| -------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| Why might polynomial regression perform poorly on extrapolation?                             | Because high-degree polynomials can produce extreme values outside the training range.                                                  |\n",
    "| How do regularization methods (Ridge/Lasso) help polynomial regression?                      | They penalize large coefficients, reducing overfitting while still modeling non-linearity.                                              |\n",
    "| How is polynomial regression different from using non-linear algorithms like decision trees? | Polynomial regression assumes a parametric polynomial relationship, while decision trees capture non-linearity in a non-parametric way. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d5d207",
   "metadata": {},
   "source": [
    "## 📘 **Theory: Ridge Regression**\n",
    "\n",
    "**Definition:**\n",
    "Ridge Regression is a type of **regularized linear regression** that adds an $L2$ penalty to the cost function.\n",
    "\n",
    "* It helps prevent **overfitting** by shrinking large coefficient values.\n",
    "* Unlike ordinary least squares (OLS), it modifies the cost function to include a penalty term proportional to the square of the coefficients.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Model Equation**\n",
    "\n",
    "$$\n",
    "J(\\beta) = \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{n} \\beta_j^2\n",
    "$$\n",
    "\n",
    "| Term                     | Meaning                                              |\n",
    "| ------------------------ | ---------------------------------------------------- |\n",
    "| $y_i$                    | Actual value                                         |\n",
    "| $\\hat{y}_i$              | Predicted value                                      |\n",
    "| $\\beta_j$                | Model coefficients                                   |\n",
    "| $\\lambda$                | Regularization parameter (controls penalty strength) |\n",
    "| $\\lambda \\sum \\beta_j^2$ | $L2$ penalty term                                    |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **How It Works**\n",
    "\n",
    "| Step | Description                                                                                                                                                                 |\n",
    "| ---- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| 1    | Adds a penalty term $\\lambda \\sum \\beta_j^2$ to the cost function.                                                                                                          |\n",
    "| 2    | This discourages large coefficient values, reducing model complexity.                                                                                                       |\n",
    "| 3    | The hyperparameter $\\lambda$ controls the trade-off: <br> - Small $\\lambda$ → behaves like OLS <br> - Large $\\lambda$ → stronger regularization (coefficients shrink more). |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Key Characteristics**\n",
    "\n",
    "| Aspect                    | Ridge Regression                                       |\n",
    "| ------------------------- | ------------------------------------------------------ |\n",
    "| Penalty Type              | $L2$ Regularization                                    |\n",
    "| Coefficient Shrinkage     | Coefficients are reduced but never set to zero         |\n",
    "| Handles Multicollinearity | Yes, it reduces variance caused by correlated features |\n",
    "| Prevents Overfitting      | Yes, by controlling model complexity                   |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                                   | Disadvantages                                            |\n",
    "| -------------------------------------------- | -------------------------------------------------------- |\n",
    "| Reduces overfitting in high-dimensional data | Does not perform feature selection (keeps all variables) |\n",
    "| Handles multicollinearity well               | Requires tuning of $\\lambda$                             |\n",
    "| Works with many correlated predictors        | Not as interpretable when $\\lambda$ is high              |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Applications**\n",
    "\n",
    "* Finance (predicting stock returns with many correlated indicators)\n",
    "* Healthcare (gene expression data where predictors are highly correlated)\n",
    "* Any high-dimensional regression problem\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                                             | Answer                                                                        |\n",
    "| ---------------------------------------------------- | ----------------------------------------------------------------------------- |\n",
    "| What is ridge regression?                            | A linear regression model with $L2$ regularization to prevent overfitting.    |\n",
    "| What does the regularization parameter $\\lambda$ do? | Controls the strength of penalty; larger $\\lambda$ shrinks coefficients more. |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                                  | Answer                                                                              |\n",
    "| --------------------------------------------------------- | ----------------------------------------------------------------------------------- |\n",
    "| How is ridge regression different from linear regression? | Ridge adds an $L2$ penalty term to control coefficient sizes, reducing overfitting. |\n",
    "| Does ridge regression perform feature selection?          | No, it shrinks coefficients but does not set them to zero (unlike Lasso).           |\n",
    "| How do you choose $\\lambda$ in ridge regression?          | Using techniques like cross-validation or grid search.                              |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                                        | Answer                                                                                         |\n",
    "| --------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- |\n",
    "| Why does ridge regression help with multicollinearity?          | It shrinks correlated feature coefficients, reducing their variance and stabilizing estimates. |\n",
    "| Can ridge regression be used when $p > n$ (features > samples)? | Yes, it performs well in high-dimensional settings by controlling variance.                    |\n",
    "| How does ridge regression relate to Bayesian statistics?        | It is equivalent to assuming a Gaussian prior on the coefficients.                             |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: Ridge Regression**\n",
    "\n",
    "```python\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X.flatten() + np.random.randn(100)  # y = 4 + 3x + noise\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Ridge Regression model with regularization parameter alpha\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "\n",
    "# Train the model\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = ridge_model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Ridge Coefficient (slope):\", ridge_model.coef_)\n",
    "print(\"Ridge Intercept:\", ridge_model.intercept_)\n",
    "print(\"Mean Squared Error on Test Set:\", mse)\n",
    "```\n",
    "\n",
    "**Output Example:**\n",
    "\n",
    "```\n",
    "Ridge Coefficient (slope): [2.90]\n",
    "Ridge Intercept: 4.17\n",
    "Mean Squared Error on Test Set: 0.83\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49a4962",
   "metadata": {},
   "source": [
    "## 📘 **Theory: Lasso Regression**\n",
    "\n",
    "**Definition:**\n",
    "Lasso Regression (**Least Absolute Shrinkage and Selection Operator**) is a type of **regularized linear regression** that uses an $L1$ penalty to shrink coefficients.\n",
    "\n",
    "* Unlike Ridge Regression, Lasso can **force some coefficients to exactly zero**, effectively performing **feature selection**.\n",
    "* This makes it useful for high-dimensional datasets with many irrelevant features.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Model Equation**\n",
    "\n",
    "The Lasso objective function is:\n",
    "\n",
    "$$\n",
    "J(\\beta) = \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{n} |\\beta_j|\n",
    "$$\n",
    "\n",
    "| Term           | Meaning                                               |   |                   |\n",
    "| -------------- | ----------------------------------------------------- | - | ----------------- |\n",
    "| $y_i$          | Actual values                                         |   |                   |\n",
    "| $\\hat{y}_i$    | Predicted values                                      |   |                   |\n",
    "| $\\beta_j$      | Model coefficients                                    |   |                   |\n",
    "| $\\lambda$      | Regularization parameter controlling penalty strength |   |                   |\n",
    "| ( \\lambda \\sum | \\beta\\_j                                              | ) | $L1$ penalty term |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **How It Works**\n",
    "\n",
    "| Step | Description                                                                                                                                       |\n",
    "| ---- | ------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| 1    | Adds $L1$ penalty to the cost function.                                                                                                           |\n",
    "| 2    | Encourages sparsity by driving some coefficients to zero.                                                                                         |\n",
    "| 3    | Hyperparameter $\\lambda$ controls the strength: <br> - Small $\\lambda$ → behaves like OLS <br> - Large $\\lambda$ → more coefficients become zero. |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Key Characteristics**\n",
    "\n",
    "| Aspect                    | Lasso Regression                                                    |\n",
    "| ------------------------- | ------------------------------------------------------------------- |\n",
    "| Penalty Type              | $L1$ Regularization                                                 |\n",
    "| Coefficient Shrinkage     | Shrinks some coefficients to zero                                   |\n",
    "| Feature Selection         | Yes, automatically removes irrelevant features                      |\n",
    "| Handles Multicollinearity | Yes, but may arbitrarily select one variable from correlated groups |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                               | Disadvantages                                   |\n",
    "| ---------------------------------------- | ----------------------------------------------- |\n",
    "| Performs feature selection automatically | Can be unstable with highly correlated features |\n",
    "| Reduces overfitting                      | May underfit when $\\lambda$ is too large        |\n",
    "| Useful for high-dimensional data         | Sensitive to scaling of variables               |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Applications**\n",
    "\n",
    "* High-dimensional datasets (e.g., genomics, text data)\n",
    "* Feature selection before training complex models\n",
    "* Preventing overfitting while simplifying models\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                                   | Answer                                                                                                             |\n",
    "| ------------------------------------------ | ------------------------------------------------------------------------------------------------------------------ |\n",
    "| What is Lasso regression?                  | A linear regression with $L1$ regularization that can shrink some coefficients to zero.                            |\n",
    "| How is it different from Ridge regression? | Ridge uses $L2$ penalty (shrinks coefficients but keeps all); Lasso uses $L1$ (can set some coefficients to zero). |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                | Answer                                                                    |\n",
    "| --------------------------------------- | ------------------------------------------------------------------------- |\n",
    "| Does Lasso perform feature selection?   | Yes, it can eliminate irrelevant features by assigning zero coefficients. |\n",
    "| How do you choose $\\lambda$ in Lasso?   | Use cross-validation to find the optimal penalty strength.                |\n",
    "| When would you prefer Lasso over Ridge? | When you suspect many irrelevant features and want a sparse model.        |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                        | Answer                                                                                                                 |\n",
    "| ----------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- |\n",
    "| How does Lasso behave with correlated features? | It tends to pick one feature and ignore the others, which may cause instability.                                       |\n",
    "| Can Lasso be combined with Ridge?               | Yes, Elastic Net combines both $L1$ and $L2$ penalties.                                                                |\n",
    "| Why does the $L1$ norm lead to sparsity?        | The geometry of the $L1$ constraint (diamond shape) leads to solutions at the axes, forcing some coefficients to zero. |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: Lasso Regression**\n",
    "\n",
    "```python\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic data: y = 4 + 3x1 + 0x2 + noise\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 2)      # Two features\n",
    "y = 4 + 3 * X[:, 0] + np.random.randn(100)  # x2 has no effect\n",
    "\n",
    "# Split into training/testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Lasso Regression model with regularization parameter alpha\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "\n",
    "# Train the model\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lasso_model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Lasso Coefficients:\", lasso_model.coef_)\n",
    "print(\"Intercept:\", lasso_model.intercept_)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "```\n",
    "\n",
    "**Example Output:**\n",
    "\n",
    "```\n",
    "Lasso Coefficients: [2.85 0.  ]\n",
    "Intercept: 4.11\n",
    "Mean Squared Error: 0.94\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d231873c",
   "metadata": {},
   "source": [
    "## 📘 **Theory: Elastic Net Regression**\n",
    "\n",
    "**Definition:**\n",
    "Elastic Net Regression is a **regularized linear regression** technique that combines both $L1$ (Lasso) and $L2$ (Ridge) penalties.\n",
    "\n",
    "* It inherits the **feature selection** property of Lasso and the **stability** of Ridge.\n",
    "* Particularly useful when predictors are highly correlated.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Model Equation**\n",
    "\n",
    "The Elastic Net objective function is:\n",
    "\n",
    "$$\n",
    "J(\\beta) = \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda_1 \\sum_{j=1}^{n} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{n} \\beta_j^2\n",
    "$$\n",
    "\n",
    "Alternatively, it is often expressed with a mixing parameter $\\alpha$:\n",
    "\n",
    "$$\n",
    "J(\\beta) = \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda \\left[ \\alpha \\sum_{j=1}^{n} |\\beta_j| + (1 - \\alpha) \\sum_{j=1}^{n} \\beta_j^2 \\right]\n",
    "$$\n",
    "\n",
    "| Term      | Meaning                                 |\n",
    "| --------- | --------------------------------------- |\n",
    "| $\\lambda$ | Overall regularization strength         |\n",
    "| $\\alpha$  | Mixing parameter (0 → Ridge, 1 → Lasso) |\n",
    "| $L1$ term | Encourages sparsity (feature selection) |\n",
    "| $L2$ term | Shrinks coefficients, reduces variance  |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **How It Works**\n",
    "\n",
    "| Step | Description                                                         |\n",
    "| ---- | ------------------------------------------------------------------- |\n",
    "| 1    | Uses both $L1$ and $L2$ penalties to control model complexity.      |\n",
    "| 2    | $\\alpha$ balances between Lasso and Ridge behavior.                 |\n",
    "| 3    | Useful when some features are irrelevant and others are correlated. |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Key Characteristics**\n",
    "\n",
    "| Aspect                    | Elastic Net                                         |\n",
    "| ------------------------- | --------------------------------------------------- |\n",
    "| Penalty Type              | Combination of $L1$ (Lasso) and $L2$ (Ridge)        |\n",
    "| Feature Selection         | Yes, like Lasso                                     |\n",
    "| Stability                 | More stable than Lasso when features are correlated |\n",
    "| Handles Multicollinearity | Yes, due to $L2$ component                          |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                           | Disadvantages                                                |\n",
    "| ------------------------------------ | ------------------------------------------------------------ |\n",
    "| Combines benefits of Lasso and Ridge | Requires tuning of two parameters ($\\lambda$ and $\\alpha$)   |\n",
    "| Handles correlated features better   | Slightly more complex to implement                           |\n",
    "| Performs feature selection           | Interpretation may be harder than standard linear regression |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Applications**\n",
    "\n",
    "* High-dimensional datasets (genomics, text mining)\n",
    "* Scenarios where feature selection and stability are both important\n",
    "* Datasets with correlated predictors\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                               | Answer                                                                         |\n",
    "| -------------------------------------- | ------------------------------------------------------------------------------ |\n",
    "| What is Elastic Net regression?        | A linear regression technique with both $L1$ and $L2$ penalties.               |\n",
    "| How does it relate to Ridge and Lasso? | It’s a hybrid: behaves like Ridge when $\\alpha=0$, like Lasso when $\\alpha=1$. |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                                              | Answer                                                                   |\n",
    "| --------------------------------------------------------------------- | ------------------------------------------------------------------------ |\n",
    "| Why is Elastic Net preferred over Lasso when features are correlated? | Because it avoids arbitrarily selecting one feature and ignoring others. |\n",
    "| How do you tune Elastic Net parameters?                               | Use cross-validation to find optimal $\\lambda$ and $\\alpha$.             |\n",
    "| Does Elastic Net perform feature selection?                           | Yes, due to the $L1$ component.                                          |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                           | Answer                                                                                              |\n",
    "| -------------------------------------------------- | --------------------------------------------------------------------------------------------------- |\n",
    "| Why is Elastic Net considered a compromise?        | It balances the sparsity of Lasso with the stability of Ridge, giving better results in many cases. |\n",
    "| How does Elastic Net behave when $p > n$?          | Performs well because the $L2$ term stabilizes the model.                                           |\n",
    "| What’s the Bayesian interpretation of Elastic Net? | It corresponds to a combination of Laplace (for L1) and Gaussian (for L2) priors on coefficients.   |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: Elastic Net Regression**\n",
    "\n",
    "```python\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic data: y = 5 + 2x1 + 3x2 + noise\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 2)\n",
    "y = 5 + 2 * X[:, 0] + 3 * X[:, 1] + np.random.randn(100)\n",
    "\n",
    "# Split into training/testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Elastic Net model (alpha controls L1/L2 mix, l1_ratio sets the balance)\n",
    "elastic_model = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "elastic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = elastic_model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Elastic Net Coefficients:\", elastic_model.coef_)\n",
    "print(\"Intercept:\", elastic_model.intercept_)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "```\n",
    "\n",
    "**Example Output:**\n",
    "\n",
    "```\n",
    "Elastic Net Coefficients: [1.95 2.90]\n",
    "Intercept: 5.10\n",
    "Mean Squared Error: 0.98\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33c3ddc",
   "metadata": {},
   "source": [
    "## 📘 **Theory: Cross-Validation**\n",
    "\n",
    "**Definition:**\n",
    "Cross-Validation (CV) is a **model evaluation technique** used to assess how well a machine learning model generalizes to unseen data.\n",
    "\n",
    "* It avoids overfitting by testing the model on multiple train-test splits.\n",
    "* The most commonly used technique is **k-fold cross-validation**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Why Use Cross-Validation?**\n",
    "\n",
    "| Reason                   | Explanation                                                |\n",
    "| ------------------------ | ---------------------------------------------------------- |\n",
    "| Avoids Overfitting       | Model is evaluated on unseen subsets repeatedly.           |\n",
    "| Better Generalization    | Uses multiple splits instead of a single train-test split. |\n",
    "| Reliable Model Selection | Helps choose the best model/hyperparameters.               |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Types of Cross-Validation**\n",
    "\n",
    "| Type                        | Description                                                                                                                              |\n",
    "| --------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **k-Fold Cross-Validation** | Data is split into $k$ equal folds; the model is trained on $k-1$ folds and tested on the remaining fold. The process repeats $k$ times. |\n",
    "| **Stratified k-Fold**       | Similar to k-fold but preserves the class proportion (important for classification).                                                     |\n",
    "| **Leave-One-Out (LOO)**     | Special case of k-fold where $k = n$; each sample is used once as test data.                                                             |\n",
    "| **Leave-p-Out (LPO)**       | Similar to LOO but leaves out $p$ samples for testing.                                                                                   |\n",
    "| **Time Series Split**       | Used for time series data; preserves order (no shuffling).                                                                               |\n",
    "| **Repeated k-Fold**         | Runs k-fold multiple times with different random splits.                                                                                 |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **How k-Fold Cross-Validation Works**\n",
    "\n",
    "1. Split dataset into $k$ equal parts (folds).\n",
    "2. For each fold:\n",
    "\n",
    "   * Train the model on $k-1$ folds.\n",
    "   * Test on the remaining fold.\n",
    "3. Average the evaluation metric across all folds.\n",
    "4. The average score gives a robust estimate of model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                               | Disadvantages                                         |\n",
    "| ---------------------------------------- | ----------------------------------------------------- |\n",
    "| More reliable estimate than single split | More computationally expensive                        |\n",
    "| Works with small datasets well           | May still have variance if data is not representative |\n",
    "| Reduces bias in performance estimation   | Complex to implement for time-series data             |\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                         | Answer                                                                                                   |\n",
    "| -------------------------------- | -------------------------------------------------------------------------------------------------------- |\n",
    "| What is cross-validation?        | A model evaluation technique that tests a model on multiple train-test splits to check generalization.   |\n",
    "| What is k-fold cross-validation? | It splits data into $k$ folds and trains/tests $k$ times, each time using a different fold as test data. |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                                       | Answer                                                                                      |\n",
    "| -------------------------------------------------------------- | ------------------------------------------------------------------------------------------- |\n",
    "| Why is cross-validation better than a single train-test split? | It uses multiple splits, giving a more accurate performance estimate and reducing variance. |\n",
    "| What is stratified k-fold cross-validation?                    | It ensures each fold maintains the same class proportion as the original dataset.           |\n",
    "| When would you use leave-one-out CV?                           | For very small datasets where every observation is critical.                                |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                               | Answer                                                                                                          |\n",
    "| ------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------- |\n",
    "| Why is cross-validation not suitable for time series?  | Because it randomly splits data, breaking temporal order; use time series split instead.                        |\n",
    "| How is cross-validation used in hyperparameter tuning? | Grid Search / Random Search with cross-validation evaluates models for each parameter set and selects the best. |\n",
    "| How does repeated cross-validation improve results?    | It reduces variance further by averaging results over multiple random splits.                                   |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: k-Fold Cross-Validation**\n",
    "\n",
    "```python\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Generate sample regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Define model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Define k-Fold Cross-Validation (k=5)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation (using R^2 score as metric)\n",
    "scores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"Cross-Validation Scores:\", scores)\n",
    "print(\"Average R^2 Score:\", np.mean(scores))\n",
    "```\n",
    "\n",
    "**Example Output:**\n",
    "\n",
    "```\n",
    "Cross-Validation Scores: [0.87 0.91 0.89 0.88 0.90]\n",
    "Average R^2 Score: 0.89\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Example: Stratified k-Fold for Classification**\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Define model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Stratified k-Fold (k=5)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
    "\n",
    "print(\"Cross-Validation Accuracy Scores:\", scores)\n",
    "print(\"Average Accuracy:\", np.mean(scores))\n",
    "```\n",
    "\n",
    "This approach works for both regression and classification problems, with appropriate metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585a3e3c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5a93062",
   "metadata": {},
   "source": [
    "## 📘 **Theory: Hyperparameter Tuning**\n",
    "\n",
    "**Definition:**\n",
    "Hyperparameter Tuning is the process of selecting the **optimal set of hyperparameters** that maximize a model’s performance.\n",
    "\n",
    "* **Hyperparameters** are parameters set **before** the learning process (e.g., learning rate, regularization strength, number of trees).\n",
    "* Proper tuning prevents underfitting or overfitting and improves generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Hyperparameters vs Parameters**\n",
    "\n",
    "| Aspect           | Model Parameters                             | Hyperparameters                               |\n",
    "| ---------------- | -------------------------------------------- | --------------------------------------------- |\n",
    "| **Definition**   | Learned from data during training            | Set manually before training                  |\n",
    "| **Examples**     | Coefficients $\\beta$ in Linear Regression    | Learning rate, $\\lambda$ in Ridge/Lasso       |\n",
    "| **Optimization** | Learned via algorithms like Gradient Descent | Tuned via search methods (Grid, Random, etc.) |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Why Hyperparameter Tuning is Important?**\n",
    "\n",
    "* Improves model accuracy and generalization.\n",
    "* Helps prevent overfitting or underfitting.\n",
    "* Ensures optimal use of model capacity.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Common Hyperparameters in ML Models**\n",
    "\n",
    "| Model                            | Hyperparameters                                       |\n",
    "| -------------------------------- | ----------------------------------------------------- |\n",
    "| **Linear Models (Ridge, Lasso)** | $\\lambda$ (regularization strength)                   |\n",
    "| **Decision Trees**               | max\\_depth, min\\_samples\\_split, min\\_samples\\_leaf   |\n",
    "| **Random Forest**                | n\\_estimators, max\\_depth, max\\_features              |\n",
    "| **Gradient Boosting**            | learning\\_rate, n\\_estimators, max\\_depth             |\n",
    "| **SVM**                          | C (regularization), kernel, gamma                     |\n",
    "| **KNN**                          | k (number of neighbors), distance metric              |\n",
    "| **Neural Networks**              | learning\\_rate, batch\\_size, epochs, number of layers |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Hyperparameter Tuning Methods**\n",
    "\n",
    "| Method                    | Description                                                                                      |\n",
    "| ------------------------- | ------------------------------------------------------------------------------------------------ |\n",
    "| **Manual Search**         | Manually selecting values based on experience.                                                   |\n",
    "| **Grid Search**           | Tries all combinations from a grid of hyperparameters; exhaustive but computationally expensive. |\n",
    "| **Random Search**         | Samples random combinations; more efficient for large search spaces.                             |\n",
    "| **Bayesian Optimization** | Uses probability models to select next best set of parameters (e.g., HyperOpt, Optuna).          |\n",
    "| **Automated Tuning**      | Tools like AutoML (TPOT, Auto-Sklearn) automate the process.                                     |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Grid Search vs Random Search**\n",
    "\n",
    "| Aspect         | Grid Search                    | Random Search                           |\n",
    "| -------------- | ------------------------------ | --------------------------------------- |\n",
    "| **Search**     | Exhaustive over parameter grid | Randomly samples parameter combinations |\n",
    "| **Efficiency** | Slow for large spaces          | Faster and often finds good solutions   |\n",
    "| **Use Case**   | Small search space             | Large search space                      |\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                          | Answer                                                                                |\n",
    "| --------------------------------- | ------------------------------------------------------------------------------------- |\n",
    "| What is hyperparameter tuning?    | It’s the process of selecting the best hyperparameters to optimize model performance. |\n",
    "| Give examples of hyperparameters. | Learning rate, number of trees, regularization strength, etc.                         |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                                      | Answer                                                                                                            |\n",
    "| ------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- |\n",
    "| What’s the difference between parameters and hyperparameters? | Parameters are learned during training; hyperparameters are set before training and control the learning process. |\n",
    "| Why use cross-validation during tuning?                       | To ensure the chosen hyperparameters generalize well on unseen data.                                              |\n",
    "| When would you prefer random search over grid search?         | When the search space is large and you want faster convergence.                                                   |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                       | Answer                                                                                         |\n",
    "| ---------------------------------------------- | ---------------------------------------------------------------------------------------------- |\n",
    "| What are the drawbacks of grid search?         | Computationally expensive and may miss good values outside the grid.                           |\n",
    "| How does Bayesian optimization improve tuning? | It uses prior performance data to choose the next best set of parameters, making it efficient. |\n",
    "| Can hyperparameter tuning cause overfitting?   | Yes, if tuning is done only on the validation set without proper cross-validation.             |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: Grid Search with Cross-Validation**\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "# Define model\n",
    "ridge = Ridge()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Grid Search with 5-Fold CV\n",
    "grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Alpha:\", grid_search.best_params_)\n",
    "print(\"Best Score (MSE):\", -grid_search.best_score_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: Randomized Search**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Define random parameter grid\n",
    "param_dist = {'alpha': np.logspace(-3, 2, 50)}\n",
    "\n",
    "# Random Search with 5-Fold CV\n",
    "random_search = RandomizedSearchCV(ridge, param_distributions=param_dist, n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "random_search.fit(X, y)\n",
    "\n",
    "print(\"Best Alpha:\", random_search.best_params_)\n",
    "print(\"Best Score (MSE):\", -random_search.best_score_)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0148fc97",
   "metadata": {},
   "source": [
    "## 📘 **Theory: Model Pickling (Saving and Loading Models)**\n",
    "\n",
    "**Definition:**\n",
    "Model Pickling refers to the process of **serializing (saving) a trained machine learning model to a file** and later **deserializing (loading) it** for future use without retraining.\n",
    "\n",
    "* It uses Python’s built-in `pickle` module or libraries like `joblib`.\n",
    "* This allows deploying the model in production or sharing it across systems.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Why Use Pickling?**\n",
    "\n",
    "| Reason               | Explanation                                                   |\n",
    "| -------------------- | ------------------------------------------------------------- |\n",
    "| **Avoid Retraining** | Saves computation time by reusing already trained models.     |\n",
    "| **Portability**      | Models can be saved and loaded on different machines.         |\n",
    "| **Deployment**       | Required for integrating models into real-world applications. |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Pickling Process**\n",
    "\n",
    "| Step | Action                                                                             |\n",
    "| ---- | ---------------------------------------------------------------------------------- |\n",
    "| 1    | Train the machine learning model.                                                  |\n",
    "| 2    | Serialize (pickle) the model into a file using `pickle.dump()` or `joblib.dump()`. |\n",
    "| 3    | Load (unpickle) the model later using `pickle.load()` or `joblib.load()`.          |\n",
    "| 4    | Use the loaded model to make predictions without retraining.                       |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Pickle vs Joblib**\n",
    "\n",
    "| Feature         | Pickle                  | Joblib                               |\n",
    "| --------------- | ----------------------- | ------------------------------------ |\n",
    "| **Best For**    | Small models            | Large models with NumPy arrays       |\n",
    "| **Performance** | Slower for big datasets | Faster, optimized for numerical data |\n",
    "| **File Size**   | Larger                  | Smaller                              |\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                | Answer                                                            |\n",
    "| ----------------------- | ----------------------------------------------------------------- |\n",
    "| What is pickling in ML? | The process of saving a trained ML model to a file for later use. |\n",
    "| Why do we use pickling? | To reuse trained models without retraining.                       |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                                   | Answer                                                                                     |\n",
    "| ---------------------------------------------------------- | ------------------------------------------------------------------------------------------ |\n",
    "| What file extension is commonly used for pickled models?   | `.pkl`                                                                                     |\n",
    "| What’s the difference between pickle and joblib?           | Joblib is optimized for large NumPy arrays and is faster.                                  |\n",
    "| Can pickled models be used in other programming languages? | No, they are Python-specific. Use formats like ONNX or PMML for cross-platform deployment. |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                                                         | Answer                                                                                          |\n",
    "| -------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |\n",
    "| What are security risks of unpickling?                                           | Pickle can execute arbitrary code during loading, so only load from trusted sources.            |\n",
    "| How do you deploy a pickled model safely?                                        | Use secure storage, versioning, and verify sources before loading.                              |\n",
    "| How is pickling different from exporting models in formats like `.h5` or `.sav`? | `.pkl` is Python-native, while `.h5` (Keras) and `.sav` (SPSS) have specific framework support. |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: Model Pickling using `pickle`**\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save (pickle) the model\n",
    "with open(\"logistic_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Load (unpickle) the model\n",
    "with open(\"logistic_model.pkl\", \"rb\") as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "# Make predictions using loaded model\n",
    "predictions = loaded_model.predict(X_test)\n",
    "print(\"Predictions:\", predictions[:5])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: Using `joblib` (Recommended for Large Models)**\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Save model using joblib\n",
    "joblib.dump(model, \"logistic_model.joblib\")\n",
    "\n",
    "# Load model using joblib\n",
    "loaded_model = joblib.load(\"logistic_model.joblib\")\n",
    "\n",
    "# Predictions\n",
    "print(\"Predictions:\", loaded_model.predict(X_test)[:5])\n",
    "```\n",
    "\n",
    "This ensures the model is **stored and reused efficiently** without retraining.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b682d40c",
   "metadata": {},
   "source": [
    "## 📘 **Theory: Logistic Regression**\n",
    "\n",
    "**Definition:**\n",
    "Logistic Regression is a **supervised learning algorithm** used for **classification** problems.\n",
    "\n",
    "* Despite its name, it is used for predicting **categorical outcomes**, not continuous ones.\n",
    "* It estimates the **probability** that a given input belongs to a particular class using the **logistic (sigmoid) function**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Model Equation**\n",
    "\n",
    "Unlike linear regression, logistic regression models the **log-odds** of the probability $p$ as a linear combination of features:\n",
    "\n",
    "$$\n",
    "\\log\\left( \\frac{p}{1 - p} \\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n\n",
    "$$\n",
    "\n",
    "The probability $p$ is then obtained using the **sigmoid function**:\n",
    "\n",
    "$$\n",
    "p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n)}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Key Characteristics**\n",
    "\n",
    "| Feature           | Description                                                    |\n",
    "| ----------------- | -------------------------------------------------------------- |\n",
    "| **Output**        | Probability between 0 and 1                                    |\n",
    "| **Decision Rule** | If $p > 0.5$ → class 1, else class 0                           |\n",
    "| **Cost Function** | Log Loss (Cross-Entropy Loss)                                  |\n",
    "| **Optimization**  | Parameters estimated using Maximum Likelihood Estimation (MLE) |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Types of Logistic Regression**\n",
    "\n",
    "| Type                                | Use Case                                                               |\n",
    "| ----------------------------------- | ---------------------------------------------------------------------- |\n",
    "| **Binary Logistic Regression**      | Classifies into two categories (e.g., spam vs not spam)                |\n",
    "| **Multinomial Logistic Regression** | Handles more than two classes without ordering (e.g., types of fruits) |\n",
    "| **Ordinal Logistic Regression**     | For ordered categories (e.g., ratings: low, medium, high)              |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                       | Disadvantages                                                  |\n",
    "| -------------------------------- | -------------------------------------------------------------- |\n",
    "| Simple, efficient, interpretable | Assumes linear relationship between features and log-odds      |\n",
    "| Outputs probabilities            | Not suitable for complex non-linear relationships              |\n",
    "| Works well on small datasets     | Can underperform with high-dimensional data unless regularized |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Applications**\n",
    "\n",
    "* Email spam detection\n",
    "* Credit scoring\n",
    "* Disease diagnosis (yes/no outcomes)\n",
    "* Customer churn prediction\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                                      | Answer                                                                      |\n",
    "| --------------------------------------------- | --------------------------------------------------------------------------- |\n",
    "| What is logistic regression used for?         | It’s used for classification, predicting probabilities of class membership. |\n",
    "| What function does it use to map predictions? | Sigmoid (logistic) function.                                                |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                                      | Answer                                                                                 |\n",
    "| ------------------------------------------------------------- | -------------------------------------------------------------------------------------- |\n",
    "| What cost function is used in logistic regression?            | Log Loss (Cross-Entropy Loss).                                                         |\n",
    "| Why can’t we use linear regression for classification?        | Because it predicts values outside \\[0,1] and doesn’t model probabilities correctly.   |\n",
    "| What’s the difference between logistic and linear regression? | Linear predicts continuous values; logistic predicts probabilities for classification. |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                                               | Answer                                                                                       |\n",
    "| ---------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- |\n",
    "| How is the decision boundary determined in logistic regression?        | It is linear in feature space, defined by $\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n = 0$. |\n",
    "| How do you handle multicollinearity in logistic regression?            | Use regularization (L1/L2) or remove correlated features.                                    |\n",
    "| Can logistic regression be extended to non-linear decision boundaries? | Yes, by adding polynomial features or using kernel logistic regression.                      |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: Logistic Regression (Binary Classification)**\n",
    "\n",
    "```python\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset (binary classification: class 0 vs class 1)\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X = X[y != 2]  # Use only two classes\n",
    "y = y[y != 2]\n",
    "\n",
    "# Split into training/testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create logistic regression model\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = log_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Example: Multinomial Logistic Regression**\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset (3 classes)\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Multinomial logistic regression\n",
    "multi_log_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=500)\n",
    "multi_log_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = multi_log_model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Multinomial Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e81e1ef",
   "metadata": {},
   "source": [
    "## 📘 **Theory: Support Vector Machine (SVM)**\n",
    "\n",
    "**Definition:**\n",
    "Support Vector Machine (SVM) is a **supervised learning algorithm** used for both **classification** and **regression** (called SVR).\n",
    "\n",
    "* It works by finding the **optimal hyperplane** that separates data into classes with the **maximum margin**.\n",
    "* The data points closest to the hyperplane are called **support vectors**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Key Concepts**\n",
    "\n",
    "| Concept             | Description                                                                  |\n",
    "| ------------------- | ---------------------------------------------------------------------------- |\n",
    "| **Hyperplane**      | Decision boundary that separates classes.                                    |\n",
    "| **Margin**          | Distance between the hyperplane and the nearest data points from each class. |\n",
    "| **Support Vectors** | Data points that lie closest to the hyperplane and influence its position.   |\n",
    "| **Kernel Trick**    | Transforms data into higher dimensions to make it linearly separable.        |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **SVM Objective Function**\n",
    "\n",
    "For classification, SVM tries to solve:\n",
    "\n",
    "$$\n",
    "\\min_{w,b} \\frac{1}{2} \\|w\\|^2 \\quad \\text{subject to } y_i(w \\cdot x_i + b) \\geq 1\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $w$ = weight vector (defines hyperplane)\n",
    "* $b$ = bias term\n",
    "* $y_i$ = class labels (+1 or -1)\n",
    "\n",
    "For **soft margin SVM**, a penalty parameter $C$ is added to allow some misclassification.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Kernel Functions**\n",
    "\n",
    "| Kernel Type                     | Description                                                                    |\n",
    "| ------------------------------- | ------------------------------------------------------------------------------ |\n",
    "| **Linear**                      | Works well for linearly separable data.                                        |\n",
    "| **Polynomial**                  | Captures polynomial relationships.                                             |\n",
    "| **RBF (Radial Basis Function)** | Handles non-linear decision boundaries by mapping to higher-dimensional space. |\n",
    "| **Sigmoid**                     | Similar to neural networks' activation.                                        |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Hyperparameters**\n",
    "\n",
    "| Parameter  | Description                                                                       |\n",
    "| ---------- | --------------------------------------------------------------------------------- |\n",
    "| **C**      | Regularization parameter (controls margin width vs. misclassification tolerance). |\n",
    "| **Kernel** | Specifies the kernel function (linear, poly, rbf, sigmoid).                       |\n",
    "| **Gamma**  | Defines influence of a single training point in RBF/poly kernels.                 |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                                       | Disadvantages                                |\n",
    "| ------------------------------------------------ | -------------------------------------------- |\n",
    "| Works well with high-dimensional data            | Sensitive to parameter selection (C, gamma)  |\n",
    "| Effective for non-linear boundaries with kernels | Computationally expensive for large datasets |\n",
    "| Robust to overfitting (with proper C)            | Not ideal for datasets with a lot of noise   |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Applications**\n",
    "\n",
    "* Text classification (spam detection)\n",
    "* Image recognition (face detection)\n",
    "* Bioinformatics (protein classification)\n",
    "* Financial forecasting\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                  | Answer                                                                                           |\n",
    "| ------------------------- | ------------------------------------------------------------------------------------------------ |\n",
    "| What is an SVM?           | A supervised algorithm that finds an optimal hyperplane to separate classes with maximum margin. |\n",
    "| What are support vectors? | Data points closest to the hyperplane, defining its position.                                    |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                           | Answer                                                                                |\n",
    "| -------------------------------------------------- | ------------------------------------------------------------------------------------- |\n",
    "| What is the role of the parameter $C$ in SVM?      | Controls trade-off between maximizing margin and minimizing misclassification errors. |\n",
    "| What is the kernel trick?                          | A method to transform data into higher dimensions to make it linearly separable.      |\n",
    "| When would you use a linear kernel vs. RBF kernel? | Linear for linearly separable data; RBF for non-linear relationships.                 |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                         | Answer                                                                                   |\n",
    "| ------------------------------------------------ | ---------------------------------------------------------------------------------------- |\n",
    "| How does SVM handle non-linearly separable data? | By using soft margins and kernel functions.                                              |\n",
    "| Why is SVM effective in high-dimensional spaces? | Because the margin is determined by support vectors, not the dimensionality.             |\n",
    "| Can SVM be used for regression?                  | Yes, Support Vector Regression (SVR) applies the same principles for continuous targets. |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: SVM for Classification**\n",
    "\n",
    "```python\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Use only two classes for binary classification\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create SVM model with RBF kernel\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Example: SVM with Linear Kernel**\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Linear kernel SVM\n",
    "linear_svm = SVC(kernel='linear', C=1.0)\n",
    "linear_svm.fit(X_train, y_train)\n",
    "print(\"Linear SVM Accuracy:\", linear_svm.score(X_test, y_test))\n",
    "```\n",
    "\n",
    "These examples show how SVM can be applied to binary classification with both **linear** and **RBF** kernels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff58ac2",
   "metadata": {},
   "source": [
    "## 📘 **Theory: Naive Bayes Theorem (Naive Bayes Classifier)**\n",
    "\n",
    "**Definition:**\n",
    "Naive Bayes is a **supervised classification algorithm** based on **Bayes’ Theorem**.\n",
    "\n",
    "* It assumes **features are conditionally independent** given the class (the “naive” assumption).\n",
    "* Despite this simplification, it performs very well in many real-world applications, especially with text data.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Bayes’ Theorem**\n",
    "\n",
    "$$\n",
    "P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}\n",
    "$$\n",
    "\n",
    "| Term   | Meaning                                |                                                          |\n",
    "| ------ | -------------------------------------- | -------------------------------------------------------- |\n",
    "| ( P(C  | X) )                                   | Posterior probability of class $C$ given features $X$    |\n",
    "| ( P(X  | C) )                                   | Likelihood – probability of features $X$ given class $C$ |\n",
    "| $P(C)$ | Prior probability of class $C$         |                                                          |\n",
    "| $P(X)$ | Evidence – probability of features $X$ |                                                          |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Naive Bayes Assumption**\n",
    "\n",
    "The features $x_1, x_2, ..., x_n$ are assumed to be **conditionally independent** given the class label $C$:\n",
    "\n",
    "$$\n",
    "P(X|C) = P(x_1|C) \\cdot P(x_2|C) \\cdot ... \\cdot P(x_n|C)\n",
    "$$\n",
    "\n",
    "This simplifies computation significantly.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Types of Naive Bayes Classifiers**\n",
    "\n",
    "| Type                        | Use Case                                                                            |\n",
    "| --------------------------- | ----------------------------------------------------------------------------------- |\n",
    "| **Gaussian Naive Bayes**    | Assumes features follow a Gaussian (normal) distribution; used for continuous data. |\n",
    "| **Multinomial Naive Bayes** | For discrete count data (e.g., word counts in text classification).                 |\n",
    "| **Bernoulli Naive Bayes**   | For binary/boolean features (e.g., presence or absence of a word).                  |\n",
    "| **Categorical Naive Bayes** | For categorical features with multiple levels.                                      |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                            | Disadvantages                                                 |\n",
    "| ------------------------------------- | ------------------------------------------------------------- |\n",
    "| Simple, fast, and efficient           | Assumes feature independence, which may not hold in real data |\n",
    "| Works well with small datasets        | Poor performance if features are highly correlated            |\n",
    "| Performs well for text classification | Estimates probabilities poorly when data is sparse            |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Applications**\n",
    "\n",
    "* Email spam detection\n",
    "* Sentiment analysis\n",
    "* Document classification\n",
    "* Medical diagnosis\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                  | Answer                                                                                                              |\n",
    "| ------------------------- | ------------------------------------------------------------------------------------------------------------------- |\n",
    "| What is Naive Bayes?      | A classification algorithm based on Bayes’ Theorem with the assumption that features are conditionally independent. |\n",
    "| Why is it called “Naive”? | Because it assumes all features contribute independently to the probability of a class.                             |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                                 | Answer                                                                                               |\n",
    "| -------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- |\n",
    "| What are the different types of Naive Bayes classifiers? | Gaussian, Multinomial, Bernoulli, and Categorical Naive Bayes.                                       |\n",
    "| When would you use Multinomial Naive Bayes?              | For text classification where features represent word counts or frequencies.                         |\n",
    "| What is Laplace smoothing in Naive Bayes?                | A technique to handle zero probabilities by adding a small constant (usually 1) to frequency counts. |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                                     | Answer                                                                                                   |\n",
    "| ------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------- |\n",
    "| How does Naive Bayes handle correlated features?             | It doesn’t handle them well because of the independence assumption.                                      |\n",
    "| Why does Naive Bayes work well despite its naive assumption? | Because in many cases, class predictions depend on dominant features, making independence less critical. |\n",
    "| Can Naive Bayes be used for continuous features?             | Yes, with Gaussian Naive Bayes which assumes a normal distribution.                                      |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: Gaussian Naive Bayes**\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Gaussian Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Example: Multinomial Naive Bayes for Text Classification**\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset (subset for speed)\n",
    "data = fetch_20newsgroups(subset='train', categories=['sci.space', 'comp.graphics'])\n",
    "X_train, y_train = data.data, data.target\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=['sci.space', 'comp.graphics'])\n",
    "X_test, y_test = data_test.data, data_test.target\n",
    "\n",
    "# Create a pipeline with vectorizer + Naive Bayes\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Text Classification Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "This covers **Naive Bayes Theory, Interview Points, and Python Implementations** for both numeric and text data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1900af5a",
   "metadata": {},
   "source": [
    "## 📘 **Theory: K-Nearest Neighbors (KNN) – Regression & Classification**\n",
    "\n",
    "**Definition:**\n",
    "K-Nearest Neighbors (KNN) is a **non-parametric, instance-based supervised learning algorithm** used for both **classification** and **regression**.\n",
    "\n",
    "* Predictions are made based on the **k** closest training samples in the feature space.\n",
    "* It does not assume any underlying data distribution (non-parametric).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **How KNN Works**\n",
    "\n",
    "| Step | Description                                                                                |\n",
    "| ---- | ------------------------------------------------------------------------------------------ |\n",
    "| 1    | Choose a value for $k$ (number of neighbors).                                              |\n",
    "| 2    | Calculate the distance (usually Euclidean) between the test point and all training points. |\n",
    "| 3    | Select the $k$ nearest neighbors.                                                          |\n",
    "| 4    | **For classification**: Predict the majority class among neighbors.                        |\n",
    "| 5    | **For regression**: Predict the average (mean) of neighbors' target values.                |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **KNN for Classification**\n",
    "\n",
    "* Assigns a class based on **majority voting** from the $k$ nearest neighbors.\n",
    "* Decision boundaries are often non-linear.\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{mode}(y_i \\, \\text{of } k \\text{ nearest neighbors})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **KNN for Regression**\n",
    "\n",
    "* Predicts a continuous value by taking the **average** (or weighted average) of the $k$ nearest neighbors’ target values.\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\frac{1}{k} \\sum_{i=1}^{k} y_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Distance Metrics**\n",
    "\n",
    "| Metric                | Formula                               |             |   |\n",
    "| --------------------- | ------------------------------------- | ----------- | - |\n",
    "| **Euclidean**         | $d = \\sqrt{\\sum (x_i - y_i)^2}$       |             |   |\n",
    "| **Manhattan**         | ( d = \\sum                            | x\\_i - y\\_i | ) |\n",
    "| **Minkowski**         | Generalization of Euclidean/Manhattan |             |   |\n",
    "| **Cosine Similarity** | Measures angular distance             |             |   |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Hyperparameters in KNN**\n",
    "\n",
    "| Parameter           | Description                                                                            |\n",
    "| ------------------- | -------------------------------------------------------------------------------------- |\n",
    "| **k**               | Number of neighbors; small $k$ → sensitive to noise, large $k$ → smoother predictions. |\n",
    "| **Weights**         | Uniform (equal weight) or distance-based (closer points have more influence).          |\n",
    "| **Distance Metric** | Euclidean (default), Manhattan, Minkowski, etc.                                        |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                                 | Disadvantages                                             |\n",
    "| ------------------------------------------ | --------------------------------------------------------- |\n",
    "| Simple, intuitive, and non-parametric      | Slow for large datasets (computes distance to all points) |\n",
    "| Works for both regression & classification | Sensitive to irrelevant features & feature scaling        |\n",
    "| No training phase (lazy learner)           | Curse of dimensionality affects performance               |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Applications**\n",
    "\n",
    "* Recommendation systems\n",
    "* Pattern recognition (handwriting, face recognition)\n",
    "* Medical diagnosis (disease classification)\n",
    "* Stock price prediction (regression)\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                 | Answer                                                                                                                                   |\n",
    "| ------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| What is KNN?             | A supervised learning algorithm that predicts based on the majority class (classification) or average (regression) of nearest neighbors. |\n",
    "| What does $k$ represent? | The number of nearest neighbors considered for prediction.                                                                               |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                       | Answer                                                                                    |\n",
    "| ---------------------------------------------- | ----------------------------------------------------------------------------------------- |\n",
    "| How do you choose the best $k$?                | Using cross-validation; typically odd values are chosen for classification to avoid ties. |\n",
    "| Why does KNN require feature scaling?          | Because it uses distance metrics, and unscaled features can dominate.                     |\n",
    "| What happens if $k$ is too small or too large? | Small $k$ → overfitting; large $k$ → underfitting.                                        |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                     | Answer                                                                      |\n",
    "| -------------------------------------------- | --------------------------------------------------------------------------- |\n",
    "| How does KNN handle high-dimensional data?   | Poorly, because distances become less meaningful (curse of dimensionality). |\n",
    "| How can you speed up KNN for large datasets? | Use KD-trees, Ball-trees, or Approximate Nearest Neighbor algorithms.       |\n",
    "| Is KNN a parametric or non-parametric model? | Non-parametric, as it doesn’t learn explicit parameters.                    |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: KNN Classification**\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Scale features (important for KNN)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create KNN Classifier (k=5)\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"KNN Classification Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: KNN Regression**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create KNN Regressor (k=5)\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = knn_regressor.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"KNN Regression MSE:\", mse)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346bbcb5",
   "metadata": {},
   "source": [
    "## 📘 **Theory: Decision Tree**\n",
    "\n",
    "**Definition:**\n",
    "A **Decision Tree** is a supervised learning algorithm used for both **classification** and **regression**.\n",
    "\n",
    "* It splits the dataset into smaller subsets based on feature conditions, forming a tree-like structure of decisions.\n",
    "* Internal nodes represent tests on features, branches represent outcomes, and leaf nodes represent final predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **How a Decision Tree Works**\n",
    "\n",
    "| Step | Description                                                                            |\n",
    "| ---- | -------------------------------------------------------------------------------------- |\n",
    "| 1    | Start with the entire dataset as the root node.                                        |\n",
    "| 2    | Choose the best feature and threshold to split data (using a criterion).               |\n",
    "| 3    | Split into subsets and repeat recursively.                                             |\n",
    "| 4    | Stop splitting when stopping criteria are met (max depth, min samples, or pure nodes). |\n",
    "| 5    | Assign class label (classification) or average value (regression) to leaf nodes.       |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Splitting Criteria**\n",
    "\n",
    "| Criterion                      | Used In        | Description                                        |\n",
    "| ------------------------------ | -------------- | -------------------------------------------------- |\n",
    "| **Gini Impurity**              | Classification | Measures impurity: lower Gini = purer node.        |\n",
    "| **Entropy (Information Gain)** | Classification | Measures information gain using entropy reduction. |\n",
    "| **Variance Reduction**         | Regression     | Splits that minimize variance within nodes.        |\n",
    "| **Mean Squared Error (MSE)**   | Regression     | Splits minimizing MSE.                             |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Key Hyperparameters**\n",
    "\n",
    "| Parameter               | Description                                                      |\n",
    "| ----------------------- | ---------------------------------------------------------------- |\n",
    "| **max\\_depth**          | Maximum depth of the tree (controls overfitting).                |\n",
    "| **min\\_samples\\_split** | Minimum samples required to split a node.                        |\n",
    "| **min\\_samples\\_leaf**  | Minimum samples at a leaf node.                                  |\n",
    "| **criterion**           | Function to measure quality of split (`gini`, `entropy`, `mse`). |\n",
    "| **max\\_features**       | Number of features to consider at each split.                    |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                           | Disadvantages                                             |\n",
    "| ------------------------------------ | --------------------------------------------------------- |\n",
    "| Easy to understand & interpret       | Prone to overfitting if not pruned                        |\n",
    "| Handles numerical & categorical data | Unstable: small changes in data can change tree structure |\n",
    "| No need for feature scaling          | May create biased trees if some classes dominate          |\n",
    "| Captures non-linear relationships    | Not as accurate as ensemble methods (e.g., Random Forest) |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Applications**\n",
    "\n",
    "* Credit risk assessment\n",
    "* Medical diagnosis\n",
    "* Customer segmentation\n",
    "* Fraud detection\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                                            | Answer                                                                          |\n",
    "| --------------------------------------------------- | ------------------------------------------------------------------------------- |\n",
    "| What is a decision tree?                            | A tree-based model that splits data into subsets to make predictions.           |\n",
    "| What splitting criteria are used in decision trees? | Gini Impurity, Entropy (for classification), and MSE/variance (for regression). |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                                  | Answer                                                                                                     |\n",
    "| --------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |\n",
    "| How do you prevent overfitting in decision trees?         | Limit tree depth, set minimum samples per leaf, or use pruning.                                            |\n",
    "| What is the difference between Gini Impurity and Entropy? | Both measure impurity, but Gini is computationally faster; entropy uses log and provides information gain. |\n",
    "| Why is feature scaling not required for decision trees?   | Because splitting is based on thresholds, not distance calculations.                                       |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                                                                          | Answer                                                                       |\n",
    "| ------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |\n",
    "| What is pruning in decision trees?                                                                | Removing branches that add little predictive power to prevent overfitting.   |\n",
    "| How does a decision tree handle missing values?                                                   | It can assign surrogate splits or use available values to decide splits.     |\n",
    "| Why are ensemble methods (Random Forest, Gradient Boosting) preferred over single decision trees? | They reduce variance and improve generalization by combining multiple trees. |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: Decision Tree Classification**\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Decision Tree Classifier\n",
    "dt_classifier = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: Decision Tree Regression**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Decision Tree Regressor\n",
    "dt_regressor = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "dt_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = dt_regressor.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Decision Tree Regression MSE:\", mse)\n",
    "```\n",
    "\n",
    "This covers **Decision Tree Theory, Interview Q\\&A, and Python Implementations** for both **classification** and **regression**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730b307f",
   "metadata": {},
   "source": [
    "## 📘 **Theory: Random Forest**\n",
    "\n",
    "**Definition:**\n",
    "Random Forest is an **ensemble learning algorithm** that builds multiple decision trees and combines their predictions to improve accuracy and reduce overfitting.\n",
    "\n",
    "* It is used for both **classification** and **regression**.\n",
    "* The final prediction is made by **majority vote** (classification) or **average** (regression) of the trees.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **How Random Forest Works**\n",
    "\n",
    "| Step | Description                                                                              |\n",
    "| ---- | ---------------------------------------------------------------------------------------- |\n",
    "| 1    | Randomly select subsets of data (bootstrap sampling) to train individual decision trees. |\n",
    "| 2    | At each split, only a random subset of features is considered (feature bagging).         |\n",
    "| 3    | Grow trees fully (no pruning), making them diverse.                                      |\n",
    "| 4    | Combine predictions: majority vote (classification) or mean (regression).                |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Why Random Forest Works Well?**\n",
    "\n",
    "* **Bagging (Bootstrap Aggregation):** Reduces variance by averaging multiple trees.\n",
    "* **Feature Randomness:** Reduces correlation between trees.\n",
    "* **Ensemble Effect:** Multiple weak learners combine to form a strong learner.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Key Hyperparameters**\n",
    "\n",
    "| Parameter               | Description                                                     |\n",
    "| ----------------------- | --------------------------------------------------------------- |\n",
    "| **n\\_estimators**       | Number of trees in the forest (more trees → better but slower). |\n",
    "| **max\\_depth**          | Maximum depth of each tree (controls overfitting).              |\n",
    "| **max\\_features**       | Number of features considered at each split.                    |\n",
    "| **min\\_samples\\_split** | Minimum samples required to split a node.                       |\n",
    "| **bootstrap**           | Whether bootstrap samples are used (default: True).             |\n",
    "| **criterion**           | Metric for split quality (`gini`, `entropy`, `mse`).            |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                                        | Disadvantages                                  |\n",
    "| ------------------------------------------------- | ---------------------------------------------- |\n",
    "| Reduces overfitting (compared to single trees)    | Less interpretable than individual trees       |\n",
    "| Works well with both numerical & categorical data | Can be slow with many trees and large datasets |\n",
    "| Handles missing data and outliers well            | Requires tuning for optimal performance        |\n",
    "| Robust to noise and high-dimensional data         | Uses more memory due to many trees             |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Applications**\n",
    "\n",
    "* Fraud detection\n",
    "* Stock market prediction\n",
    "* Customer churn analysis\n",
    "* Feature importance ranking\n",
    "* Medical diagnosis\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                                        | Answer                                                                                          |\n",
    "| ----------------------------------------------- | ----------------------------------------------------------------------------------------------- |\n",
    "| What is Random Forest?                          | An ensemble algorithm that combines multiple decision trees to improve prediction accuracy.     |\n",
    "| How does it differ from a single decision tree? | Random Forest averages multiple trees to reduce overfitting, whereas a single tree may overfit. |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                         | Answer                                                                                              |\n",
    "| ------------------------------------------------ | --------------------------------------------------------------------------------------------------- |\n",
    "| What is bagging in Random Forest?                | Bootstrap Aggregation – training each tree on a random sample with replacement.                     |\n",
    "| How does Random Forest handle feature selection? | At each split, only a random subset of features is considered, increasing diversity.                |\n",
    "| What’s the role of `n_estimators`?               | It controls the number of trees; more trees generally improve performance but increase computation. |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                           | Answer                                                                  |\n",
    "| -------------------------------------------------- | ----------------------------------------------------------------------- |\n",
    "| How does Random Forest compute feature importance? | By measuring how much each feature decreases impurity across all trees. |\n",
    "| Can Random Forest handle imbalanced datasets?      | Yes, by adjusting class weights or using balanced subsampling.          |\n",
    "| Why is Random Forest less prone to overfitting?    | Because averaging multiple uncorrelated trees reduces variance.         |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: Random Forest Classification**\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: Random Forest Regression**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Random Forest Regression MSE:\", mse)\n",
    "```\n",
    "\n",
    "This covers **Random Forest Theory, Key Interview Points, and Implementations** for both **classification** and **regression**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91251810",
   "metadata": {},
   "source": [
    "## 📘 **Theory: AdaBoost (Adaptive Boosting)**\n",
    "\n",
    "**Definition:**\n",
    "AdaBoost (**Adaptive Boosting**) is an **ensemble learning technique** that combines multiple **weak learners** (usually shallow decision trees called decision stumps) to create a **strong classifier**.\n",
    "\n",
    "* It assigns **weights** to training instances, giving higher weight to misclassified samples in subsequent iterations.\n",
    "* Final predictions are a **weighted vote** (classification) or a **weighted average** (regression) of all weak learners.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **How AdaBoost Works**\n",
    "\n",
    "| Step | Description                                                                             |\n",
    "| ---- | --------------------------------------------------------------------------------------- |\n",
    "| 1    | Train a weak learner (e.g., decision stump) on the data.                                |\n",
    "| 2    | Calculate the error rate ($e$) of the model.                                            |\n",
    "| 3    | Increase weights of misclassified samples, so the next learner focuses on harder cases. |\n",
    "| 4    | Assign a weight $\\alpha$ to each weak learner based on its accuracy.                    |\n",
    "| 5    | Combine predictions using a weighted vote.                                              |\n",
    "| 6    | Repeat until the specified number of learners is reached.                               |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Mathematical Intuition**\n",
    "\n",
    "Each weak learner $h_t(x)$ is assigned a weight $\\alpha_t$ based on its error $e_t$:\n",
    "\n",
    "$$\n",
    "\\alpha_t = \\frac{1}{2} \\ln \\left( \\frac{1 - e_t}{e_t} \\right)\n",
    "$$\n",
    "\n",
    "The final classifier $H(x)$ is:\n",
    "\n",
    "$$\n",
    "H(x) = \\text{sign} \\left( \\sum_{t=1}^{T} \\alpha_t h_t(x) \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Key Hyperparameters**\n",
    "\n",
    "| Parameter           | Description                                                                                |\n",
    "| ------------------- | ------------------------------------------------------------------------------------------ |\n",
    "| **n\\_estimators**   | Number of weak learners to combine.                                                        |\n",
    "| **learning\\_rate**  | Shrinks the contribution of each learner (trade-off between underfitting and overfitting). |\n",
    "| **base\\_estimator** | The weak learner (default is `DecisionTreeClassifier(max_depth=1)`).                       |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                                            | Disadvantages                               |\n",
    "| ----------------------------------------------------- | ------------------------------------------- |\n",
    "| Reduces both bias and variance                        | Sensitive to noisy data and outliers        |\n",
    "| Works well with simple models (e.g., decision stumps) | Requires careful parameter tuning           |\n",
    "| Performs well on many classification problems         | Can overfit if too many estimators are used |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Applications**\n",
    "\n",
    "* Face detection (Viola–Jones algorithm uses AdaBoost)\n",
    "* Fraud detection\n",
    "* Medical diagnosis\n",
    "* Text classification\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                     | Answer                                                                                                      |\n",
    "| ---------------------------- | ----------------------------------------------------------------------------------------------------------- |\n",
    "| What is AdaBoost?            | An ensemble method that builds a strong classifier by combining multiple weak learners with weighted votes. |\n",
    "| Why is it called \"adaptive\"? | Because it adaptively adjusts weights to focus on misclassified samples in subsequent iterations.           |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                              | Answer                                                                                     |\n",
    "| ----------------------------------------------------- | ------------------------------------------------------------------------------------------ |\n",
    "| What type of learners are typically used in AdaBoost? | Decision stumps (single-split decision trees).                                             |\n",
    "| What is the role of the learning rate in AdaBoost?    | Controls the contribution of each weak learner; smaller values require more estimators.    |\n",
    "| How does AdaBoost handle errors?                      | It increases weights for misclassified instances so the next learner focuses more on them. |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                          | Answer                                                                                                                        |\n",
    "| ------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- |\n",
    "| Why is AdaBoost sensitive to outliers?            | Outliers are repeatedly misclassified, leading the model to assign them high weights and overfit.                             |\n",
    "| Can AdaBoost be used for regression?              | Yes, through **AdaBoostRegressor**, which minimizes a loss like least absolute deviation.                                     |\n",
    "| How does AdaBoost compare with Gradient Boosting? | AdaBoost updates sample weights based on misclassification, while Gradient Boosting minimizes a differentiable loss function. |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: AdaBoost Classification**\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create AdaBoost Classifier with decision stumps as weak learners\n",
    "adaboost_clf = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = adaboost_clf.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: AdaBoost Regression**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic regression data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create AdaBoost Regressor\n",
    "adaboost_reg = AdaBoostRegressor(n_estimators=50, learning_rate=0.8, random_state=42)\n",
    "adaboost_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = adaboost_reg.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"AdaBoost Regression MSE:\", mse)\n",
    "```\n",
    "\n",
    "This explanation covers **AdaBoost Theory, Mathematical Intuition, Interview Questions, and Implementations** for both **classification** and **regression**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536d5ed3",
   "metadata": {},
   "source": [
    "## 📘 **Theory: Gradient Boosting**\n",
    "\n",
    "**Definition:**\n",
    "Gradient Boosting is an **ensemble learning technique** that builds models sequentially, where each new model tries to **correct the errors** of the previous one.\n",
    "\n",
    "* It combines weak learners (usually decision trees) into a strong predictive model.\n",
    "* Unlike AdaBoost, which adjusts sample weights, Gradient Boosting minimizes a **loss function** using **gradient descent**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **How Gradient Boosting Works**\n",
    "\n",
    "| Step | Description                                                                              |\n",
    "| ---- | ---------------------------------------------------------------------------------------- |\n",
    "| 1    | Start with an initial model (often a constant value, e.g., mean of targets).             |\n",
    "| 2    | Compute residuals (errors) from the current model.                                       |\n",
    "| 3    | Train a new weak learner on these residuals.                                             |\n",
    "| 4    | Update the model by adding the new learner’s predictions, scaled by a **learning rate**. |\n",
    "| 5    | Repeat steps 2–4 for a specified number of iterations (`n_estimators`).                  |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Mathematical Formulation**\n",
    "\n",
    "For prediction $F(x)$:\n",
    "\n",
    "$$\n",
    "F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $F_{m-1}(x)$ → previous model\n",
    "* $h_m(x)$ → new weak learner\n",
    "* $\\eta$ → learning rate (controls step size)\n",
    "\n",
    "The model minimizes a differentiable loss function $L(y, F(x))$ using gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Key Hyperparameters**\n",
    "\n",
    "| Parameter          | Description                                                                                    |\n",
    "| ------------------ | ---------------------------------------------------------------------------------------------- |\n",
    "| **n\\_estimators**  | Number of weak learners (trees) to combine.                                                    |\n",
    "| **learning\\_rate** | Shrinks contribution of each tree to prevent overfitting.                                      |\n",
    "| **max\\_depth**     | Depth of individual decision trees.                                                            |\n",
    "| **subsample**      | Fraction of samples used for training each tree (introduces randomness to reduce overfitting). |\n",
    "| **loss**           | Loss function (e.g., deviance for classification, squared\\_error for regression).              |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                                       | Disadvantages                              |\n",
    "| ------------------------------------------------ | ------------------------------------------ |\n",
    "| High accuracy due to sequential error correction | Slower to train than bagging methods       |\n",
    "| Handles various loss functions (flexible)        | Prone to overfitting if not tuned properly |\n",
    "| Works well with structured/tabular data          | Sensitive to noise and outliers            |\n",
    "| Feature importance can be extracted              | Requires careful parameter tuning          |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Applications**\n",
    "\n",
    "* Credit scoring\n",
    "* Fraud detection\n",
    "* Ranking algorithms (e.g., search engines)\n",
    "* Customer churn prediction\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                           | Answer                                                                                                                   |\n",
    "| ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------ |\n",
    "| What is Gradient Boosting?         | An ensemble method where trees are added sequentially to minimize a loss function using gradient descent.                |\n",
    "| How is it different from AdaBoost? | AdaBoost updates sample weights, while Gradient Boosting fits new models to residual errors by minimizing loss directly. |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                                   | Answer                                                                                                  |\n",
    "| ---------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- |\n",
    "| What’s the role of the learning rate in Gradient Boosting? | It controls how much each tree contributes; lower values improve generalization but require more trees. |\n",
    "| How do you prevent overfitting in Gradient Boosting?       | Use a small learning rate, limit tree depth, use subsampling, and apply early stopping.                 |\n",
    "| Which loss functions are supported?                        | For regression: MSE, MAE; for classification: log-loss, deviance.                                       |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                              | Answer                                                                                                         |\n",
    "| ----------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- |\n",
    "| Why does Gradient Boosting outperform simple bagging? | Because it corrects previous errors sequentially instead of averaging independent models.                      |\n",
    "| How does subsampling help in Gradient Boosting?       | It introduces randomness (like in Random Forest), reducing overfitting.                                        |\n",
    "| Compare Gradient Boosting with XGBoost.               | XGBoost is an optimized, regularized version of Gradient Boosting with faster training and better performance. |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: Gradient Boosting Classification**\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: Gradient Boosting Regression**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic regression data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Gradient Boosting Regressor\n",
    "gb_reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = gb_reg.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Gradient Boosting Regression MSE:\", mse)\n",
    "```\n",
    "\n",
    "This explanation covers **Gradient Boosting Theory, How It Works, Interview Q\\&A, and Implementations** for both **classification** and **regression**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba43225a",
   "metadata": {},
   "source": [
    "## 📘 **Theory: XGBoost (Extreme Gradient Boosting)**\n",
    "\n",
    "**Definition:**\n",
    "XGBoost (**Extreme Gradient Boosting**) is an advanced implementation of Gradient Boosting that is optimized for **speed, accuracy, and scalability**.\n",
    "\n",
    "* It includes **regularization**, **parallelization**, and **sparse data handling**, making it one of the most powerful algorithms for structured/tabular data.\n",
    "* Widely used in **Kaggle competitions** and industry applications due to its high performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **How XGBoost Works**\n",
    "\n",
    "XGBoost follows the same boosting principle as Gradient Boosting but with enhancements:\n",
    "\n",
    "1. **Builds trees sequentially**, where each new tree corrects errors of the previous ones.\n",
    "2. Uses **second-order gradient (Hessian)** to optimize the loss function faster.\n",
    "3. Adds **regularization** (L1 & L2) to prevent overfitting.\n",
    "4. Supports **parallelized tree construction** for faster training.\n",
    "5. Handles **missing values** and **sparse features** efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Mathematical Objective**\n",
    "\n",
    "XGBoost minimizes:\n",
    "\n",
    "$$\n",
    "Obj = \\sum_i l(y_i, \\hat{y}_i) + \\sum_k \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $l(y_i, \\hat{y}_i)$ → differentiable loss function (e.g., log-loss, MSE)\n",
    "* $\\Omega(f_k) = \\gamma T + \\frac{1}{2} \\lambda \\| w \\|^2$ → regularization term controlling tree complexity\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Key Features of XGBoost**\n",
    "\n",
    "* Uses **Gradient Boosting framework** with second-order optimization\n",
    "* **Regularization** (L1 & L2) to avoid overfitting\n",
    "* **Column subsampling** like Random Forest (improves diversity)\n",
    "* **Early stopping** support\n",
    "* Handles **missing data automatically**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Key Hyperparameters**\n",
    "\n",
    "| Parameter                | Description                                               |\n",
    "| ------------------------ | --------------------------------------------------------- |\n",
    "| **n\\_estimators**        | Number of boosting rounds (trees).                        |\n",
    "| **learning\\_rate (eta)** | Step size shrinkage to prevent overfitting (default=0.3). |\n",
    "| **max\\_depth**           | Depth of trees; higher → more complex model.              |\n",
    "| **subsample**            | Fraction of samples used per tree (default=1).            |\n",
    "| **colsample\\_bytree**    | Fraction of features used per tree.                       |\n",
    "| **gamma**                | Minimum loss reduction required to make a further split.  |\n",
    "| **reg\\_lambda**          | L2 regularization term.                                   |\n",
    "| **reg\\_alpha**           | L1 regularization term.                                   |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                                   | Disadvantages                                         |\n",
    "| -------------------------------------------- | ----------------------------------------------------- |\n",
    "| Extremely fast and scalable                  | More complex than simple Gradient Boosting            |\n",
    "| High accuracy with proper tuning             | Prone to overfitting if hyperparameters are not tuned |\n",
    "| Built-in regularization prevents overfitting | Requires careful parameter tuning                     |\n",
    "| Handles missing values and sparse data       | Computationally expensive for very large datasets     |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Applications**\n",
    "\n",
    "* Kaggle competition-winning models\n",
    "* Fraud detection\n",
    "* Credit scoring\n",
    "* Customer churn prediction\n",
    "* Click-through rate (CTR) prediction\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                                             | Answer                                                                                                         |\n",
    "| ---------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- |\n",
    "| What is XGBoost?                                     | An optimized version of Gradient Boosting with regularization and high performance.                            |\n",
    "| Why is it better than traditional Gradient Boosting? | It uses regularization, second-order optimization, parallel processing, and better handling of missing values. |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                                 | Answer                                                                                          |\n",
    "| -------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |\n",
    "| What is the role of `gamma` in XGBoost?                  | It controls whether a node should be split; higher gamma makes the algorithm more conservative. |\n",
    "| How does XGBoost prevent overfitting?                    | Through L1/L2 regularization, subsampling, and learning rate control.                           |\n",
    "| What’s the difference between XGBoost and Random Forest? | Random Forest uses bagging, while XGBoost uses boosting with sequential error correction.       |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                                  | Answer                                                                                                                            |\n",
    "| --------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| How does XGBoost handle missing values?                   | It learns the best direction to handle missing values during training.                                                            |\n",
    "| Why is XGBoost faster than traditional Gradient Boosting? | It uses histogram-based split finding and parallelization.                                                                        |\n",
    "| Compare XGBoost with LightGBM and CatBoost.               | LightGBM is faster with large datasets, CatBoost handles categorical features better, and XGBoost is more mature and widely used. |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: XGBoost Classification**\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create XGBoost Classifier\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: XGBoost Regression**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# Generate synthetic regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create XGBoost Regressor\n",
    "xgb_reg = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = xgb_reg.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"XGBoost Regression MSE:\", mse)\n",
    "```\n",
    "\n",
    "This explanation covers **XGBoost Theory, Key Features, Interview Questions, and Python Implementations** for both **classification** and **regression**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd5a12",
   "metadata": {},
   "source": [
    "## 📘 **Theory: Unsupervised Machine Learning**\n",
    "\n",
    "**Definition:**\n",
    "Unsupervised Machine Learning is a type of machine learning where the algorithm learns patterns from **unlabeled data**.\n",
    "\n",
    "* Unlike supervised learning, there are **no predefined output labels**.\n",
    "* The algorithm tries to **find hidden structures** or **groupings** in the data.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Key Characteristics**\n",
    "\n",
    "| Feature           | Description                                              |\n",
    "| ----------------- | -------------------------------------------------------- |\n",
    "| **Training Data** | Unlabeled (no target variable).                          |\n",
    "| **Goal**          | Find patterns, clusters, or reduce dimensions.           |\n",
    "| **Common Tasks**  | Clustering, dimensionality reduction, anomaly detection. |\n",
    "| **Evaluation**    | Harder to evaluate (no ground truth labels).             |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Types of Unsupervised Learning**\n",
    "\n",
    "| Category                      | Algorithms                               | Purpose                                                               |\n",
    "| ----------------------------- | ---------------------------------------- | --------------------------------------------------------------------- |\n",
    "| **Clustering**                | K-Means, Hierarchical Clustering, DBSCAN | Groups data into clusters based on similarity.                        |\n",
    "| **Dimensionality Reduction**  | PCA, t-SNE, Autoencoders                 | Reduces feature space while preserving important information.         |\n",
    "| **Association Rule Learning** | Apriori, FP-Growth                       | Finds relationships between variables (e.g., market basket analysis). |\n",
    "| **Anomaly Detection**         | Isolation Forest, One-Class SVM          | Identifies unusual patterns or outliers.                              |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **Common Algorithms**\n",
    "\n",
    "| Algorithm                              | Type                     | Description                                                           |\n",
    "| -------------------------------------- | ------------------------ | --------------------------------------------------------------------- |\n",
    "| **K-Means**                            | Clustering               | Partitions data into $k$ clusters minimizing within-cluster variance. |\n",
    "| **Hierarchical Clustering**            | Clustering               | Builds a tree of clusters (dendrogram).                               |\n",
    "| **DBSCAN**                             | Clustering               | Density-based; identifies clusters of varying shapes and noise.       |\n",
    "| **PCA (Principal Component Analysis)** | Dimensionality Reduction | Projects data into fewer dimensions capturing maximum variance.       |\n",
    "| **t-SNE**                              | Dimensionality Reduction | Non-linear technique for visualizing high-dimensional data.           |\n",
    "| **Autoencoders**                       | Dimensionality Reduction | Neural network-based method for feature extraction.                   |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                           | Disadvantages                                      |\n",
    "| ------------------------------------ | -------------------------------------------------- |\n",
    "| Useful when labels are not available | Hard to evaluate accuracy                          |\n",
    "| Finds hidden patterns in data        | Results may be subjective                          |\n",
    "| Reduces data complexity (e.g., PCA)  | May require domain knowledge to interpret clusters |\n",
    "| Good for exploratory analysis        | Sensitive to parameter tuning                      |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Applications**\n",
    "\n",
    "* Customer segmentation\n",
    "* Market basket analysis\n",
    "* Fraud/anomaly detection\n",
    "* Image compression & feature extraction\n",
    "* Recommendation systems\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                       | Answer                                                                                      |\n",
    "| ------------------------------ | ------------------------------------------------------------------------------------------- |\n",
    "| What is unsupervised learning? | A type of machine learning where algorithms learn from unlabeled data to identify patterns. |\n",
    "| Name some examples.            | K-Means clustering, PCA, DBSCAN, Autoencoders.                                              |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                         | Answer                                                                                        |\n",
    "| ------------------------------------------------ | --------------------------------------------------------------------------------------------- |\n",
    "| How is clustering different from classification? | Clustering groups data without labels, while classification assigns labels based on training. |\n",
    "| What is the main goal of PCA?                    | To reduce dimensionality while retaining most variance in the data.                           |\n",
    "| How do you evaluate clustering results?          | Using metrics like Silhouette Score, Davies-Bouldin Index, or visual inspection.              |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                                        | Answer                                                                                            |\n",
    "| --------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- |\n",
    "| Why is choosing $k$ in K-Means important?                       | Because $k$ determines the number of clusters; use Elbow Method or Silhouette Score to select it. |\n",
    "| How does DBSCAN handle noise better than K-Means?               | DBSCAN detects outliers as noise points and can form clusters of arbitrary shape.                 |\n",
    "| Can unsupervised learning be combined with supervised learning? | Yes, semi-supervised learning uses a mix of labeled and unlabeled data.                           |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: K-Means Clustering**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, _ = make_blobs(n_samples=200, centers=3, cluster_std=1.0, random_state=42)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# Plot clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X')\n",
    "plt.title(\"K-Means Clustering\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Example: PCA for Dimensionality Reduction**\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Apply PCA (reduce to 2 dimensions)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot PCA result\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='rainbow')\n",
    "plt.title(\"PCA Dimensionality Reduction\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "This covers **Unsupervised ML Theory, Algorithms, Interview Q\\&A, and Python Implementations** for clustering and dimensionality reduction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8af38a",
   "metadata": {},
   "source": [
    "## 📘 **Theory: Principal Component Analysis (PCA)**\n",
    "\n",
    "**Definition:**\n",
    "Principal Component Analysis (PCA) is an **unsupervised dimensionality reduction technique** that transforms high-dimensional data into a smaller number of **uncorrelated variables** called **principal components**, while retaining most of the variance.\n",
    "\n",
    "* It helps reduce complexity, improve visualization, and remove noise.\n",
    "* Often used as a **preprocessing step** in machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **How PCA Works**\n",
    "\n",
    "| Step | Description                                                                                                      |\n",
    "| ---- | ---------------------------------------------------------------------------------------------------------------- |\n",
    "| 1    | Standardize the dataset (mean=0, variance=1) to remove scale effects.                                            |\n",
    "| 2    | Compute the **covariance matrix** of the data.                                                                   |\n",
    "| 3    | Perform **eigen decomposition** (or SVD) to find eigenvectors (directions) and eigenvalues (variance explained). |\n",
    "| 4    | Sort eigenvectors by eigenvalues in descending order.                                                            |\n",
    "| 5    | Select top $k$ eigenvectors to form new feature space (principal components).                                    |\n",
    "| 6    | Transform original data into this reduced space.                                                                 |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Mathematical Formulation**\n",
    "\n",
    "Given a dataset $X$ with $n$ features:\n",
    "\n",
    "1. Compute covariance matrix:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{n} X^T X\n",
    "$$\n",
    "\n",
    "2. Solve eigenvalue problem:\n",
    "\n",
    "$$\n",
    "\\Sigma v = \\lambda v\n",
    "$$\n",
    "\n",
    "3. Principal components $v$ are eigenvectors with largest eigenvalues $\\lambda$.\n",
    "4. Transform data:\n",
    "\n",
    "$$\n",
    "Z = XW\n",
    "$$\n",
    "\n",
    "where $W$ is the matrix of top $k$ eigenvectors.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Key Points**\n",
    "\n",
    "| Aspect                       | Description                                                   |\n",
    "| ---------------------------- | ------------------------------------------------------------- |\n",
    "| **Principal Component**      | A new axis capturing maximum variance in data.                |\n",
    "| **Explained Variance Ratio** | Shows how much information (variance) each component retains. |\n",
    "| **Orthogonality**            | Principal components are orthogonal (uncorrelated).           |\n",
    "| **Number of Components**     | Chosen to capture a high percentage (e.g., 95%) of variance.  |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                        | Disadvantages                                         |\n",
    "| --------------------------------- | ----------------------------------------------------- |\n",
    "| Reduces dimensionality and noise  | Harder to interpret transformed features              |\n",
    "| Improves computational efficiency | May lose some information                             |\n",
    "| Removes multicollinearity         | Linear method, cannot capture nonlinear relationships |\n",
    "| Useful for visualization in 2D/3D | Scaling is required before applying PCA               |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Applications**\n",
    "\n",
    "* Data compression\n",
    "* Noise reduction\n",
    "* Feature extraction before classification/regression\n",
    "* Visualization of high-dimensional data (e.g., images, genetics, NLP features)\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                         | Answer                                                                                                |\n",
    "| -------------------------------- | ----------------------------------------------------------------------------------------------------- |\n",
    "| What is PCA?                     | A dimensionality reduction method that projects data onto orthogonal axes capturing maximum variance. |\n",
    "| Why is scaling important in PCA? | Because PCA is sensitive to feature magnitudes; scaling ensures all features contribute equally.      |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                            | Answer                                                                                                                  |\n",
    "| --------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- |\n",
    "| What are eigenvalues and eigenvectors in PCA?       | Eigenvectors define directions (principal components), and eigenvalues represent variance captured by those directions. |\n",
    "| How do you decide the number of components to keep? | Use explained variance ratio and choose components that capture most variance (e.g., 95%).                              |\n",
    "| Does PCA work for categorical data?                 | Not directly; data must be numeric (encoding is needed).                                                                |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                               | Answer                                                                                          |\n",
    "| -------------------------------------- | ----------------------------------------------------------------------------------------------- |\n",
    "| How does PCA handle multicollinearity? | It transforms correlated variables into uncorrelated components, removing multicollinearity.    |\n",
    "| Can PCA improve model accuracy?        | Sometimes, by removing noise and reducing dimensionality, but may also lose useful information. |\n",
    "| How is PCA different from LDA?         | PCA is unsupervised (uses variance), while LDA is supervised (uses class separability).         |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: PCA on Iris Dataset**\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Step 1: Standardize features\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Step 2: Apply PCA (reduce to 2 components for visualization)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Explained variance\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Step 3: Plot results\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='rainbow')\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"PCA on Iris Dataset\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Example: PCA for Dimensionality Reduction Before Classification**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply PCA (keep 95% variance)\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Train logistic regression on reduced data\n",
    "clf = LogisticRegression(max_iter=500)\n",
    "clf.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print(\"Accuracy after PCA:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "This covers **PCA Theory, Steps, Mathematical Intuition, Interview Points, and Python Implementations** for both visualization and dimensionality reduction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275cfdd",
   "metadata": {},
   "source": [
    "## 📘 **Theory: K-Means Clustering**\n",
    "\n",
    "**Definition:**\n",
    "K-Means is an **unsupervised machine learning algorithm** used for **clustering** data into $k$ distinct, non-overlapping groups.\n",
    "\n",
    "* It tries to minimize the **within-cluster variance** (distance between points and their cluster centroid).\n",
    "* It is one of the most popular clustering algorithms due to its simplicity and efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **How K-Means Works**\n",
    "\n",
    "| Step | Description                                                                    |\n",
    "| ---- | ------------------------------------------------------------------------------ |\n",
    "| 1    | Choose $k$ (number of clusters).                                               |\n",
    "| 2    | Initialize $k$ cluster centroids randomly.                                     |\n",
    "| 3    | Assign each data point to the nearest centroid (cluster assignment).           |\n",
    "| 4    | Update centroids by computing the mean of assigned points.                     |\n",
    "| 5    | Repeat steps 3–4 until centroids no longer change significantly (convergence). |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Mathematical Objective**\n",
    "\n",
    "K-Means minimizes the **Sum of Squared Errors (SSE)**:\n",
    "\n",
    "$$\n",
    "J = \\sum_{i=1}^{k} \\sum_{x_j \\in C_i} \\| x_j - \\mu_i \\|^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $k$ = number of clusters\n",
    "* $x_j$ = data point\n",
    "* $\\mu_i$ = centroid of cluster $i$\n",
    "* $C_i$ = set of points in cluster $i$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Key Parameters**\n",
    "\n",
    "| Parameter     | Description                                                   |\n",
    "| ------------- | ------------------------------------------------------------- |\n",
    "| **k**         | Number of clusters (user-defined).                            |\n",
    "| **init**      | Initialization method (`k-means++` is preferred).             |\n",
    "| **max\\_iter** | Maximum number of iterations.                                 |\n",
    "| **n\\_init**   | Number of times K-Means is run with different centroid seeds. |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **How to Choose $k$?**\n",
    "\n",
    "* **Elbow Method:** Plot SSE vs $k$ and look for an \"elbow\" point where the curve bends.\n",
    "* **Silhouette Score:** Measures how similar a point is to its cluster compared to others.\n",
    "* **Gap Statistics:** Compares performance with a reference random dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                             | Disadvantages                      |\n",
    "| -------------------------------------- | ---------------------------------- |\n",
    "| Simple, fast, and easy to implement    | Requires specifying $k$ in advance |\n",
    "| Works well on large datasets           | Sensitive to outliers and noise    |\n",
    "| Handles numerical data effectively     | Only works with spherical clusters |\n",
    "| Scales well with the number of samples | Sensitive to feature scaling       |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Applications**\n",
    "\n",
    "* Customer segmentation\n",
    "* Image compression\n",
    "* Market segmentation\n",
    "* Document clustering\n",
    "* Pattern recognition\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                          | Answer                                                                                                      |\n",
    "| --------------------------------- | ----------------------------------------------------------------------------------------------------------- |\n",
    "| What is K-Means?                  | An unsupervised clustering algorithm that partitions data into $k$ clusters based on distance to centroids. |\n",
    "| How does K-Means assign clusters? | By minimizing the distance between points and cluster centroids.                                            |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                     | Answer                                                                 |\n",
    "| -------------------------------------------- | ---------------------------------------------------------------------- |\n",
    "| What distance metric does K-Means use?       | Typically Euclidean distance.                                          |\n",
    "| Why is feature scaling important in K-Means? | Because it uses distances, and unscaled features can bias clustering.  |\n",
    "| How do you handle the issue of local minima? | Use multiple initializations (`n_init`) or `k-means++` initialization. |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                       | Answer                                                                                          |\n",
    "| ---------------------------------------------- | ----------------------------------------------------------------------------------------------- |\n",
    "| What are limitations of K-Means?               | Fails with non-spherical clusters, varying cluster sizes, or clusters with different densities. |\n",
    "| How is K-Means++ better than standard K-Means? | It initializes centroids to be farther apart, reducing poor clustering.                         |\n",
    "| Can K-Means be used for categorical data?      | Not directly; use K-Modes or K-Prototypes instead.                                              |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: K-Means Clustering**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.7, random_state=42)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, random_state=42)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# Plot clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', s=50)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
    "            s=200, c='red', marker='X', label='Centroids')\n",
    "plt.title(\"K-Means Clustering\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Example: Finding Optimal $k$ with Elbow Method**\n",
    "\n",
    "```python\n",
    "sse = []\n",
    "for k in range(1, 10):\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    km.fit(X)\n",
    "    sse.append(km.inertia_)  # Inertia = SSE\n",
    "\n",
    "plt.plot(range(1, 10), sse, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('SSE')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "This covers **K-Means Theory, Objective, Interview Q\\&A, and Python Implementation** with an example of how to determine the optimal number of clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c665c",
   "metadata": {},
   "source": [
    "## 📘 **Theory: Hierarchical Clustering**\n",
    "\n",
    "**Definition:**\n",
    "Hierarchical Clustering is an **unsupervised learning algorithm** that builds a hierarchy (tree structure) of clusters instead of creating a fixed number of clusters like K-Means.\n",
    "\n",
    "* It produces a **dendrogram**, a tree-like diagram showing how clusters merge or split.\n",
    "* Two main types: **Agglomerative** (bottom-up) and **Divisive** (top-down).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Types of Hierarchical Clustering**\n",
    "\n",
    "| Type                    | Description                                                                                                                             |\n",
    "| ----------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Agglomerative (AHC)** | Starts with each data point as its own cluster and iteratively merges the closest clusters until only one remains. (Most commonly used) |\n",
    "| **Divisive**            | Starts with all data points in one cluster and splits them recursively into smaller clusters.                                           |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **How Agglomerative Clustering Works**\n",
    "\n",
    "1. Treat each data point as a separate cluster.\n",
    "2. Calculate pairwise distances (Euclidean, Manhattan, etc.).\n",
    "3. Merge the two clusters that are closest together.\n",
    "4. Recalculate distances between new clusters and remaining ones.\n",
    "5. Repeat until all points are in a single cluster.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Linkage Methods (How Distance is Measured Between Clusters)**\n",
    "\n",
    "| Linkage Method       | Description                                             |\n",
    "| -------------------- | ------------------------------------------------------- |\n",
    "| **Single Linkage**   | Distance between the closest points in two clusters.    |\n",
    "| **Complete Linkage** | Distance between the farthest points in two clusters.   |\n",
    "| **Average Linkage**  | Average distance between all points in two clusters.    |\n",
    "| **Ward’s Method**    | Minimizes total within-cluster variance (most popular). |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Key Characteristics**\n",
    "\n",
    "* No need to pre-specify the number of clusters (but you must choose where to cut the dendrogram).\n",
    "* Produces a full clustering hierarchy.\n",
    "* Suitable for small to medium datasets (computationally expensive for large datasets).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                                           | Disadvantages                                 |\n",
    "| ---------------------------------------------------- | --------------------------------------------- |\n",
    "| No need to specify $k$ upfront                       | High computational complexity $O(n^2)$        |\n",
    "| Produces a hierarchy (dendrogram) for interpretation | Sensitive to noise and outliers               |\n",
    "| Works with different distance metrics                | Cannot handle very large datasets efficiently |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Applications**\n",
    "\n",
    "* Gene expression analysis (bioinformatics)\n",
    "* Customer segmentation\n",
    "* Document/topic clustering\n",
    "* Hierarchical taxonomy creation\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                         | Answer                                                                                         |\n",
    "| -------------------------------- | ---------------------------------------------------------------------------------------------- |\n",
    "| What is hierarchical clustering? | A clustering algorithm that builds a tree of clusters, merging or splitting them step by step. |\n",
    "| What is a dendrogram?            | A tree-like diagram that shows the hierarchical relationship among clusters.                   |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                                             | Answer                                                                                                  |\n",
    "| -------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- |\n",
    "| Difference between agglomerative and divisive clustering?            | Agglomerative merges small clusters into larger ones; divisive splits large clusters into smaller ones. |\n",
    "| How do you decide the number of clusters in hierarchical clustering? | By cutting the dendrogram at a certain height where large vertical gaps exist.                          |\n",
    "| Which linkage method is best?                                        | Ward’s method often performs well because it minimizes variance within clusters.                        |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                                   | Answer                                                                                                           |\n",
    "| ---------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- |\n",
    "| Why is hierarchical clustering computationally expensive?  | It requires computing and updating a full distance matrix for all data points at each step.                      |\n",
    "| Can hierarchical clustering handle non-spherical clusters? | Yes, better than K-Means in many cases.                                                                          |\n",
    "| How do you scale hierarchical clustering for big data?     | Use approximate algorithms or methods like BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies). |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: Hierarchical Clustering (Agglomerative)**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "X, _ = make_blobs(n_samples=200, centers=3, cluster_std=0.7, random_state=42)\n",
    "\n",
    "# Apply Agglomerative Clustering\n",
    "hc = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "labels = hc.fit_predict(X)\n",
    "\n",
    "# Plot results\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='rainbow', s=50)\n",
    "plt.title(\"Hierarchical Clustering (Agglomerative)\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Example: Dendrogram with SciPy**\n",
    "\n",
    "```python\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Perform hierarchical clustering using Ward's method\n",
    "Z = linkage(X, method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(10, 5))\n",
    "dendrogram(Z)\n",
    "plt.title(\"Dendrogram for Hierarchical Clustering\")\n",
    "plt.xlabel(\"Data Points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "This covers **Hierarchical Clustering Theory, Linkage Methods, Interview Q\\&A, and Python Implementations** (with Agglomerative and Dendrogram visualization).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a93e29",
   "metadata": {},
   "source": [
    "## 📘 **Theory: DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**\n",
    "\n",
    "**Definition:**\n",
    "DBSCAN is an **unsupervised clustering algorithm** that groups together data points that are **close to each other (high density)** and separates **low-density points** as noise or outliers.\n",
    "\n",
    "* Unlike K-Means, it **does not require specifying the number of clusters**.\n",
    "* It can form clusters of **arbitrary shapes**, making it robust for complex datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Key Concepts**\n",
    "\n",
    "| Term                      | Description                                                                            |\n",
    "| ------------------------- | -------------------------------------------------------------------------------------- |\n",
    "| **Core Point**            | A point with at least `min_samples` neighbors within a radius `eps`.                   |\n",
    "| **Border Point**          | Has fewer than `min_samples` neighbors but is within the neighborhood of a core point. |\n",
    "| **Noise Point (Outlier)** | Not a core point and not within `eps` distance of any core point.                      |\n",
    "| **eps (ε)**               | Maximum radius to consider two points as neighbors.                                    |\n",
    "| **min\\_samples**          | Minimum number of points required to form a dense region (including the point itself). |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **How DBSCAN Works**\n",
    "\n",
    "1. Pick an unvisited point.\n",
    "2. If it has at least `min_samples` neighbors within `eps`, mark it as a **core point** and form a cluster.\n",
    "3. Expand the cluster by including all density-reachable points.\n",
    "4. Points not belonging to any cluster are marked as **noise**.\n",
    "5. Repeat until all points are visited.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Mathematical Objective**\n",
    "\n",
    "DBSCAN does not minimize a cost function like K-Means.\n",
    "Instead, it identifies clusters by finding **connected dense regions** in the data.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Key Hyperparameters**\n",
    "\n",
    "| Parameter        | Description                                                                                             |\n",
    "| ---------------- | ------------------------------------------------------------------------------------------------------- |\n",
    "| **eps**          | Defines the neighborhood radius. Smaller `eps` → more clusters; larger `eps` → fewer clusters.          |\n",
    "| **min\\_samples** | Minimum points to form a dense region. Typically `min_samples ≈ D+1` (where `D` is number of features). |\n",
    "| **metric**       | Distance metric used (default is Euclidean).                                                            |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                                         | Disadvantages                                     |\n",
    "| -------------------------------------------------- | ------------------------------------------------- |\n",
    "| Does not require pre-specifying number of clusters | Sensitive to choice of `eps` and `min_samples`    |\n",
    "| Can find clusters of arbitrary shape               | Struggles with varying density clusters           |\n",
    "| Automatically detects outliers                     | Computationally expensive for very large datasets |\n",
    "| Works well with noisy data                         | Needs scaling for high-dimensional data           |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Applications**\n",
    "\n",
    "* Anomaly detection (fraud, network intrusion)\n",
    "* Geographic clustering (location-based data)\n",
    "* Image segmentation\n",
    "* Social network analysis\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                                         | Answer                                                                                                       |\n",
    "| ------------------------------------------------ | ------------------------------------------------------------------------------------------------------------ |\n",
    "| What is DBSCAN?                                  | A density-based clustering algorithm that groups together closely packed points and marks outliers as noise. |\n",
    "| Does DBSCAN require number of clusters as input? | No, it determines clusters automatically based on density.                                                   |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                | Answer                                                                             |\n",
    "| --------------------------------------- | ---------------------------------------------------------------------------------- |\n",
    "| What are the main parameters in DBSCAN? | `eps` (radius) and `min_samples` (minimum neighbors to form a cluster).            |\n",
    "| How does DBSCAN handle outliers?        | Points not belonging to any cluster are labeled as noise.                          |\n",
    "| Why is DBSCAN better than K-Means?      | It can detect arbitrarily shaped clusters and does not require pre-specifying $k$. |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                             | Answer                                                                                 |\n",
    "| ---------------------------------------------------- | -------------------------------------------------------------------------------------- |\n",
    "| What happens if `eps` is too small or too large?     | Too small → many small clusters/noise; too large → merges all points into one cluster. |\n",
    "| Can DBSCAN handle clusters with different densities? | Not well; HDBSCAN (Hierarchical DBSCAN) is better for varying densities.               |\n",
    "| What is the complexity of DBSCAN?                    | $O(n \\log n)$ with efficient indexing, otherwise $O(n^2)$.                             |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: DBSCAN Clustering**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.5, random_state=42)\n",
    "\n",
    "# Apply DBSCAN\n",
    "db = DBSCAN(eps=0.5, min_samples=5)\n",
    "labels = db.fit_predict(X)\n",
    "\n",
    "# Plot clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='plasma', s=50)\n",
    "plt.title(\"DBSCAN Clustering\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Example: Detecting Outliers with DBSCAN**\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Generate dataset with noise\n",
    "X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "\n",
    "# DBSCAN with parameters tuned for non-linear shape\n",
    "db = DBSCAN(eps=0.2, min_samples=5)\n",
    "labels = db.fit_predict(X)\n",
    "\n",
    "# Plot clusters (outliers labeled as -1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
    "plt.title(\"DBSCAN with Outlier Detection\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "This summary includes **DBSCAN Theory, Key Concepts, Interview Q\\&A, and Python Implementation** for clustering and anomaly detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315073ab",
   "metadata": {},
   "source": [
    "## 📘 **Theory: Silhouette Score (Silhouette Coefficient)**\n",
    "\n",
    "**Definition:**\n",
    "The **Silhouette Score** is a **metric used to evaluate the quality of clustering**.\n",
    "\n",
    "* It measures **how similar** a data point is to its own cluster (cohesion) compared to other clusters (separation).\n",
    "* The score ranges from **-1 to +1**:\n",
    "\n",
    "  * **+1** → Perfectly assigned, well-separated cluster\n",
    "  * **0** → Points lie on the boundary between clusters\n",
    "  * **-1** → Wrongly assigned, closer to another cluster than its own\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Mathematical Formula**\n",
    "\n",
    "For a data point $i$:\n",
    "\n",
    "$$\n",
    "s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $a(i)$ = average distance between $i$ and all other points in the same cluster (**intra-cluster distance**)\n",
    "* $b(i)$ = minimum average distance from $i$ to points in another cluster (**nearest-cluster distance**)\n",
    "\n",
    "The overall silhouette score is the average of $s(i)$ across all data points.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Key Points**\n",
    "\n",
    "* Works for **any clustering algorithm** (K-Means, DBSCAN, Hierarchical, etc.).\n",
    "* Higher silhouette score indicates better-defined clusters.\n",
    "* Used to help **select the optimal number of clusters $k$**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                                       | Disadvantages                                        |\n",
    "| ------------------------------------------------ | ---------------------------------------------------- |\n",
    "| Easy to interpret and compare clustering quality | Computationally expensive for large datasets         |\n",
    "| Works without ground truth labels                | Less informative when clusters have irregular shapes |\n",
    "| Can be used to determine optimal $k$             | Sensitive to distance metric choice                  |\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                         | Answer                                                                              |\n",
    "| -------------------------------- | ----------------------------------------------------------------------------------- |\n",
    "| What is the silhouette score?    | A metric that evaluates how well each data point fits within its cluster.           |\n",
    "| What is a good silhouette score? | Typically > 0.5 indicates good clustering, while < 0 suggests incorrect clustering. |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                             | Answer                                                                                                          |\n",
    "| ---------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- |\n",
    "| How is silhouette different from inertia in K-Means? | Inertia measures within-cluster sum of squares, while silhouette also considers separation from other clusters. |\n",
    "| Can silhouette score be used with DBSCAN?            | Yes, but points labeled as noise (-1) may affect the score.                                                     |\n",
    "| Does silhouette score depend on distance metric?     | Yes, because it’s based on distances between points.                                                            |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                                              | Answer                                                                                                  |\n",
    "| --------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- |\n",
    "| How does silhouette help choose optimal $k$ in K-Means?               | Compute the score for different $k$; the value with the highest score indicates the best cluster count. |\n",
    "| Why might silhouette be misleading for clusters with varying density? | Because it assumes clusters are compact and well-separated.                                             |\n",
    "| Can silhouette be negative for all points?                            | Yes, if clustering is poor and points are closer to other clusters than their own.                      |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: Silhouette Score with K-Means**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.6, random_state=42)\n",
    "\n",
    "# Apply K-Means with k=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Calculate Silhouette Score\n",
    "score = silhouette_score(X, labels)\n",
    "print(\"Silhouette Score (k=3):\", score)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Example: Finding Optimal $k$ using Silhouette Score**\n",
    "\n",
    "```python\n",
    "scores = []\n",
    "k_values = range(2, 8)\n",
    "\n",
    "for k in k_values:\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = km.fit_predict(X)\n",
    "    scores.append(silhouette_score(X, labels))\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.plot(k_values, scores, marker='o')\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Score for Optimal k\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Example: Silhouette Score with DBSCAN**\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Apply DBSCAN\n",
    "db = DBSCAN(eps=0.5, min_samples=5)\n",
    "labels = db.fit_predict(X)\n",
    "\n",
    "# Compute silhouette score (excluding noise if any)\n",
    "if len(set(labels)) > 1 and -1 not in set(labels):\n",
    "    print(\"Silhouette Score (DBSCAN):\", silhouette_score(X, labels))\n",
    "else:\n",
    "    print(\"Silhouette Score may not be meaningful (due to noise points).\")\n",
    "```\n",
    "\n",
    "This provides **Silhouette Theory, Formula, Interview Questions, and Python Implementation** for evaluating clustering performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb05554",
   "metadata": {},
   "source": [
    "## 📘 **Theory: Anomaly Detection**\n",
    "\n",
    "**Definition:**\n",
    "Anomaly Detection is the process of identifying **data points, events, or observations** that deviate significantly from the majority of the data — also called **outliers** or **novelties**.\n",
    "\n",
    "* It is often framed as an **unsupervised learning** or **semi-supervised learning** task because anomalies are rare and often not labeled.\n",
    "* Critical for domains like fraud detection, network security, fault diagnosis, and health monitoring.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Types of Anomalies**\n",
    "\n",
    "| Type                     | Description                                                                              |\n",
    "| ------------------------ | ---------------------------------------------------------------------------------------- |\n",
    "| **Point Anomalies**      | Individual data points that are unusual compared to the rest.                            |\n",
    "| **Contextual Anomalies** | Points that are anomalies in a specific context or condition (e.g., time series data).   |\n",
    "| **Collective Anomalies** | A collection of data points that together are anomalous, though individually may not be. |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Common Approaches**\n",
    "\n",
    "| Method                      | Description                                                                                             |\n",
    "| --------------------------- | ------------------------------------------------------------------------------------------------------- |\n",
    "| **Statistical Methods**     | Use statistical tests assuming a distribution (e.g., z-score, Gaussian).                                |\n",
    "| **Distance-based Methods**  | Anomalies are far from normal points (e.g., k-NN, LOF).                                                 |\n",
    "| **Density-based Methods**   | Identify points in low-density regions (e.g., DBSCAN, Local Outlier Factor).                            |\n",
    "| **Isolation-based Methods** | Isolate anomalies faster due to their unique attributes (e.g., Isolation Forest).                       |\n",
    "| **Model-based Methods**     | Use supervised/semi-supervised models trained to detect deviations (e.g., Autoencoders, One-Class SVM). |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Key Algorithms**\n",
    "\n",
    "| Algorithm                      | Type                 | Use Case                                  |\n",
    "| ------------------------------ | -------------------- | ----------------------------------------- |\n",
    "| **Z-Score**                    | Statistical          | Simple thresholding on standardized data. |\n",
    "| **Local Outlier Factor (LOF)** | Density-based        | Finds anomalies in varying densities.     |\n",
    "| **Isolation Forest**           | Isolation-based      | Efficient for high-dimensional data.      |\n",
    "| **One-Class SVM**              | Model-based          | Learns boundary around normal data.       |\n",
    "| **Autoencoders**               | Neural network-based | Reconstruction error signals anomalies.   |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Challenges**\n",
    "\n",
    "* Defining what constitutes “normal” and “anomalous.”\n",
    "* Imbalanced datasets with very few anomalies.\n",
    "* Anomalies can be context-dependent.\n",
    "* High-dimensional data complicates detection.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Interview Insights**\n",
    "\n",
    "### ✅ **Basic Level**\n",
    "\n",
    "| Question                            | Answer                                                                         |\n",
    "| ----------------------------------- | ------------------------------------------------------------------------------ |\n",
    "| What is anomaly detection?          | Identifying rare or unusual patterns that do not conform to expected behavior. |\n",
    "| Why is anomaly detection important? | To detect fraud, failures, or security breaches early.                         |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Intermediate Level**\n",
    "\n",
    "| Question                                      | Answer                                                                                                  |\n",
    "| --------------------------------------------- | ------------------------------------------------------------------------------------------------------- |\n",
    "| What are common anomaly detection techniques? | Statistical, distance-based, density-based, isolation-based, and model-based methods.                   |\n",
    "| How do you evaluate anomaly detection?        | Using metrics like Precision, Recall, F1-Score, ROC-AUC, or confusion matrix if labeled data available. |\n",
    "| What is Local Outlier Factor?                 | A method that detects anomalies by comparing local density of a point to that of neighbors.             |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Advanced Level**\n",
    "\n",
    "| Question                                                           | Answer                                                                               |\n",
    "| ------------------------------------------------------------------ | ------------------------------------------------------------------------------------ |\n",
    "| How does Isolation Forest work?                                    | It isolates anomalies by randomly partitioning data; anomalies require fewer splits. |\n",
    "| What are challenges of anomaly detection in high-dimensional data? | Curse of dimensionality reduces distance meaningfulness and increases noise.         |\n",
    "| How do autoencoders detect anomalies?                              | By training to reconstruct normal data; high reconstruction error indicates anomaly. |\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Simple Python Example: Isolation Forest**\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data\n",
    "rng = np.random.RandomState(42)\n",
    "X_normal = 0.3 * rng.randn(100, 2)\n",
    "X_outliers = rng.uniform(low=-4, high=4, size=(10, 2))\n",
    "X = np.r_[X_normal + 2, X_normal - 2, X_outliers]\n",
    "\n",
    "# Fit Isolation Forest\n",
    "clf = IsolationForest(contamination=0.1, random_state=42)\n",
    "clf.fit(X)\n",
    "pred = clf.predict(X)  # 1 for normal, -1 for anomaly\n",
    "\n",
    "print(\"Anomaly labels:\", pred)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 **Example: Local Outlier Factor**\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Fit LOF\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "labels = lof.fit_predict(X)  # 1 for inlier, -1 for outlier\n",
    "\n",
    "print(\"LOF labels:\", labels)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "This summary provides **Anomaly Detection theory, methods, interview Q\\&A, and Python examples** with practical algorithms like Isolation Forest and LOF.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c9df58",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
