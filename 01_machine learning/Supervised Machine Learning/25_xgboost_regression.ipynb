{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d79f401e",
   "metadata": {},
   "source": [
    "# XGBoost Regression: Advanced Theory & Interview Q&A\n",
    "\n",
    "## Theory\n",
    "XGBoost Regression applies the XGBoost algorithm to regression problems, leveraging regularized gradient boosting for high accuracy and efficiency. It supports custom loss functions, handles missing data, and is highly scalable. XGBoost Regression is known for its performance in competitions and real-world applications.\n",
    "\n",
    "| Aspect                | Details                                                                 |\n",
    "|----------------------|-------------------------------------------------------------------------|\n",
    "| Algorithm            | Gradient boosting with regularization                                   |\n",
    "| Loss Function        | MSE, MAE, Huber, customizable                                           |\n",
    "| Optimization         | Second-order gradient descent, parallelized                             |\n",
    "| Regularization       | L1 (Lasso), L2 (Ridge), shrinkage, subsampling                         |\n",
    "| Strengths            | Fast, accurate, robust to missing data, regularization prevents overfit |\n",
    "| Weaknesses           | Complex tuning, resource intensive, can overfit if not regularized      |\n",
    "\n",
    "## Advanced Interview Q&A\n",
    "**Q1: How does XGBoost Regression differ from Random Forest Regression?**\n",
    "A1: XGBoost builds trees sequentially with boosting and regularization, while Random Forest builds trees independently and averages their predictions.\n",
    "\n",
    "**Q2: What is the advantage of using second-order gradients in XGBoost?**\n",
    "A2: Second-order gradients (Hessian) provide more accurate updates, improving convergence and model performance.\n",
    "\n",
    "**Q3: How does XGBoost handle large datasets for regression?**\n",
    "A3: XGBoost uses out-of-core computation, parallelization, and efficient memory usage to scale to large datasets.\n",
    "\n",
    "**Q4: What strategies help prevent overfitting in XGBoost Regression?**\n",
    "A4: Use regularization, early stopping, subsampling, and careful tuning of tree depth and learning rate.\n",
    "\n",
    "**Q5: How do you interpret feature importance in XGBoost Regression?**\n",
    "A5: XGBoost provides gain, cover, and frequency metrics to assess feature importance, helping to understand model decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09062506",
   "metadata": {},
   "source": [
    "# XGBoost Regression — Theory & Interview Q&A\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is a scalable, efficient implementation of gradient boosting for regression, with advanced regularization and parallelization.\n",
    "\n",
    "| Aspect                | Details                                                                 |\n",
    "|-----------------------|------------------------------------------------------------------------|\n",
    "| **Definition**        | Efficient, scalable gradient boosting for regression.                    |\n",
    "| **Equation**          | Combines weak learners by minimizing regularized loss function          |\n",
    "| **Use Cases**         | Price prediction, time series, environmental modeling                   |\n",
    "| **Assumptions**       | Weak learners perform slightly better than random guessing              |\n",
    "| **Pros**              | High accuracy, fast, regularization, handles missing data               |\n",
    "| **Cons**              | Complex, many parameters, prone to overfitting                          |\n",
    "| **Key Parameters**    | n_estimators, learning_rate, max_depth, subsample, colsample_bytree    |\n",
    "| **Evaluation Metrics**| MSE, RMSE, R² Score                                                     |\n",
    "\n",
    "## Interview Q&A\n",
    "\n",
    "**Q1: What is XGBoost Regression?**  \n",
    "A: An efficient, scalable implementation of gradient boosting with advanced features for regression.\n",
    "\n",
    "**Q2: What are the advantages of XGBoost Regression?**  \n",
    "A: High accuracy, fast, regularization, handles missing data.\n",
    "\n",
    "**Q3: What is regularization in XGBoost?**  \n",
    "A: Penalizes model complexity to prevent overfitting.\n",
    "\n",
    "**Q4: What are the limitations?**  \n",
    "A: Complex, many parameters, prone to overfitting.\n",
    "\n",
    "**Q5: How do you prevent overfitting in XGBoost Regression?**  \n",
    "A: Use regularization, early stopping, limit tree depth.\n",
    "\n",
    "**Q6: How do you evaluate XGBoost Regression?**  \n",
    "A: Using MSE, RMSE, and R² score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e445fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 2️⃣ Load Dataset\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# 3️⃣ Split Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 4️⃣ Create Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Optional for tree-based models\n",
    "    ('xgb', XGBRegressor(random_state=42, objective='reg:squarederror'))\n",
    "])\n",
    "\n",
    "# 5️⃣ Hyperparameter Tuning\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': [100, 200, 300],\n",
    "    'xgb__max_depth': [3, 4, 5],\n",
    "    'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'xgb__subsample': [0.7, 0.8, 1],\n",
    "    'xgb__colsample_bytree': [0.7, 0.8, 1]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 6️⃣ Evaluate Best Model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"R2 Score:\", r2_score(y_test, y_pred))\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# 7️⃣ Feature Importance Visualization\n",
    "importances = best_model.named_steps['xgb'].feature_importances_\n",
    "feat_imp_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n",
    "feat_imp_df = feat_imp_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feat_imp_df, palette='coolwarm')\n",
    "plt.title(\"Feature Importance in XGBoost Regressor\")\n",
    "plt.show()\n",
    "\n",
    "# 8️⃣ Predicted vs Actual Visualization\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6, color='teal')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"XGBoost Regressor: Predicted vs Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1280fca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
