{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2e78640",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2890c8dd",
   "metadata": {},
   "source": [
    "##                                          Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Theoretical Intuition\n",
    "Deep Learning is a subset of Machine Learning inspired by the structure and function of the human brain (Artificial Neural Networks).\n",
    "\n",
    "- **Machine Learning vs. Deep Learning**:  \n",
    "  - ML: Uses algorithms to parse data, learn from it, and make decisions.  \n",
    "  - DL: Uses multiple layers of neural networks to automatically learn representations and features from raw data.  \n",
    "\n",
    "- **Key Idea**:  \n",
    "  Instead of hand-crafting features, deep learning models **learn the features** from data (e.g., images, text, audio).  \n",
    "\n",
    "- **Example**:  \n",
    "  In image recognition:  \n",
    "  * Shallow ML: Manually extract edges, textures → feed to classifier  \n",
    "  * Deep Learning: Raw pixels → network learns edges → shapes → objects  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Use Cases of Deep Learning\n",
    "- Computer Vision (face recognition, self-driving cars)  \n",
    "- Natural Language Processing (chatbots, translation, sentiment analysis)  \n",
    "- Speech Recognition (voice assistants)  \n",
    "- Healthcare (disease detection from scans, drug discovery)  \n",
    "- Finance (fraud detection, algorithmic trading)  \n",
    "- Recommendation Systems (Netflix, YouTube, Amazon)  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Mathematical Intuition (High-Level)\n",
    "Deep Learning ≈ Function Approximation  \n",
    "\n",
    "- Given input **X**, output **Y**  \n",
    "- We want to learn a function **f** such that:  \n",
    "\n",
    "\\[\n",
    "f(X; \\theta) \\approx Y\n",
    "\\]\n",
    "\n",
    "where **θ** are the parameters (weights & biases).  \n",
    "\n",
    "A neural network is basically:  \n",
    "\n",
    "\\[\n",
    "\\text{Input} \\; \\rightarrow \\; (W \\cdot X + b) \\; \\rightarrow \\; \\text{Activation} \\; \\rightarrow \\; \\text{Output}\n",
    "\\]\n",
    "\n",
    "Stack multiple layers → \"Deep\" network.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Interview Q&A\n",
    "\n",
    "| Level        | Question | Answer |\n",
    "|--------------|----------|--------|\n",
    "| Beginner     | What is Deep Learning? | A subset of ML that uses multi-layered neural networks to learn data representations. |\n",
    "| Beginner     | How is Deep Learning different from Machine Learning? | ML often needs manual feature engineering; DL automatically learns features with neural nets. |\n",
    "| Beginner     | What is a Neural Network? | A network of interconnected nodes (neurons) that transform inputs to outputs using weights, biases, and activations. |\n",
    "| Intermediate | Why do we need activation functions? | They introduce non-linearity, allowing the network to learn complex mappings. |\n",
    "| Intermediate | What are some challenges of Deep Learning? | High computation cost, need for large labeled datasets, interpretability, risk of overfitting. |\n",
    "| Intermediate | Why does deep learning need large datasets? | Because millions of parameters must be tuned; large datasets help avoid overfitting. |\n",
    "| Intermediate | Explain overfitting in deep learning. | When the model memorizes training data but fails to generalize to new data. |\n",
    "| Advanced     | How is backpropagation used in training? | It computes gradients of loss w.r.t weights using chain rule and updates them via optimization (like SGD). |\n",
    "| Advanced     | What is the vanishing gradient problem? | When gradients shrink as they are backpropagated, making deep networks hard to train. |\n",
    "| Advanced     | Difference between shallow and deep networks? | Shallow: 1–2 layers; Deep: many layers that learn hierarchical features. |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Quick Demo: Simple Neural Network on Dummy Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa540337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss: 0.0009748904267325997\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dummy dataset (y = 2x + 1)\n",
    "X = torch.linspace(-1, 1, 100).reshape(-1, 1)\n",
    "y = 2*X + 1\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Initialize\n",
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Train\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(\"Final Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d80ce5",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Theoretical Intuition\n",
    "- A **Perceptron** is the **simplest type of artificial neural network** (ANN).  \n",
    "- It is a **binary classifier** that maps input features to an output using a **linear combination of weights** and an **activation function**.  \n",
    "- Introduced by **Frank Rosenblatt (1958)**.  \n",
    "- Think of it as a **single neuron in the brain**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Key Pointers\n",
    "- It works for **linearly separable data**.  \n",
    "- Inputs (**x1, x2, ... xn**) are multiplied by weights (**w1, w2, ... wn**) and summed up with a **bias** term.  \n",
    "- Output is passed through a **step activation function** (0 or 1).  \n",
    "- **Learning**: weights are updated based on **prediction error** using the **Perceptron learning rule**.  \n",
    "- Limitation: Cannot solve **non-linear problems** like XOR.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Use Cases\n",
    "- Early **binary classification tasks**  \n",
    "- Simple **pattern recognition**  \n",
    "- Foundation for **more complex neural networks**  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Mathematical Intuition\n",
    "- Weighted sum:  \n",
    "\n",
    "\\[\n",
    "z = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b\n",
    "\\]\n",
    "\n",
    "- Activation function (step):  \n",
    "\n",
    "\\[\n",
    "y =\n",
    "\\begin{cases} \n",
    "1 & \\text{if } z \\ge 0 \\\\\n",
    "0 & \\text{if } z < 0\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "- Weight update (Perceptron learning rule):  \n",
    "\n",
    "\\[\n",
    "w_i = w_i + \\Delta w_i\n",
    "\\]  \n",
    "\n",
    "\\[\n",
    "\\Delta w_i = \\eta (y_{\\text{true}} - y_{\\text{pred}}) x_i\n",
    "\\]  \n",
    "\n",
    "where **η** = learning rate, **y_true** = actual label, **y_pred** = predicted label.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Interview Q&A\n",
    "\n",
    "| Question | Answer |\n",
    "|----------|--------|\n",
    "| What is a Perceptron? | The simplest type of neural network; a binary classifier that uses weighted sum and activation. |\n",
    "| Who introduced the Perceptron? | Frank Rosenblatt in 1958. |\n",
    "| How does a Perceptron make predictions? | Computes weighted sum of inputs + bias, passes through step function. |\n",
    "| What is the main limitation of a Perceptron? | Cannot solve non-linear problems like XOR. |\n",
    "| What is the Perceptron learning rule? | Updates weights based on prediction error: w_i = w_i + η*(y_true - y_pred)*x_i |\n",
    "| What type of data can a Perceptron classify? | Linearly separable data. |\n",
    "| Why do we need bias in a Perceptron? | Allows shifting the decision boundary away from the origin. |\n",
    "| Can a single Perceptron be used for multi-class classification? | No, only binary; multi-class requires multiple Perceptrons or other architectures. |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Code Demo: Simple Perceptron in Python\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Step activation function\n",
    "def step(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "# Perceptron class\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, lr=0.1, epochs=10):\n",
    "        self.weights = np.zeros(input_size)\n",
    "        self.bias = 0\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def predict(self, x):\n",
    "        z = np.dot(x, self.weights) + self.bias\n",
    "        return step(z)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.epochs):\n",
    "            for xi, yi in zip(X, y):\n",
    "                y_pred = self.predict(xi)\n",
    "                update = self.lr * (yi - y_pred)\n",
    "                self.weights += update * xi\n",
    "                self.bias += update\n",
    "\n",
    "# Training data: AND gate\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([0,0,0,1])\n",
    "\n",
    "# Train Perceptron\n",
    "p = Perceptron(input_size=2)\n",
    "p.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "for xi in X:\n",
    "    print(f\"Input: {xi}, Predicted: {p.predict(xi)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f56214",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
