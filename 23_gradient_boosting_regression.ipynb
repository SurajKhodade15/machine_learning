{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6288c7",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regression: Advanced Theory & Interview Q&A\n",
    "\n",
    "## Theory\n",
    "Gradient Boosting Regression is an ensemble technique that builds models sequentially, where each new model attempts to correct the errors of the previous ones. It uses decision trees as weak learners and optimizes a loss function using gradient descent. Key concepts include learning rate, number of estimators, subsampling, and regularization.\n",
    "\n",
    "| Aspect                | Details                                                                 |\n",
    "|----------------------|-------------------------------------------------------------------------|\n",
    "| Algorithm            | Sequential ensemble of weak learners (trees)                             |\n",
    "| Loss Function        | Customizable (MSE, MAE, Huber, etc.)                                     |\n",
    "| Optimization         | Gradient descent                                                         |\n",
    "| Regularization       | Shrinkage (learning rate), subsampling, tree depth, min samples split    |\n",
    "| Strengths            | Handles complex data, robust to outliers, flexible loss functions        |\n",
    "| Weaknesses           | Prone to overfitting, computationally intensive, sensitive to parameters |\n",
    "\n",
    "## Advanced Interview Q&A\n",
    "**Q1: How does Gradient Boosting differ from AdaBoost?**\n",
    "A1: AdaBoost adjusts sample weights to focus on misclassified points, while Gradient Boosting fits new models to the residuals of previous models using gradient descent on a loss function.\n",
    "\n",
    "**Q2: What is the role of the learning rate in Gradient Boosting?**\n",
    "A2: The learning rate controls the contribution of each tree to the final model. Lower values improve generalization but require more trees.\n",
    "\n",
    "**Q3: How can you prevent overfitting in Gradient Boosting?**\n",
    "A3: Use early stopping, reduce tree depth, increase min samples split, apply subsampling, and tune the learning rate.\n",
    "\n",
    "**Q4: Explain the concept of stochastic gradient boosting.**\n",
    "A4: Stochastic gradient boosting introduces randomness by subsampling the data for each tree, improving generalization and reducing overfitting.\n",
    "\n",
    "**Q5: How do you select the optimal number of estimators?**\n",
    "A5: Use cross-validation and early stopping to find the point where additional trees no longer improve validation performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b5f56e",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regression — Theory & Interview Q&A\n",
    "\n",
    "Gradient Boosting Regression is an ensemble learning method that builds models sequentially, each correcting the errors of the previous, using decision trees as weak learners to predict continuous outcomes.\n",
    "\n",
    "| Aspect                | Details                                                                 |\n",
    "|-----------------------|------------------------------------------------------------------------|\n",
    "| **Definition**        | Sequentially builds models to minimize errors, using decision trees.     |\n",
    "| **Equation**          | Combines weak learners by minimizing loss function                      |\n",
    "| **Use Cases**         | Price prediction, time series, environmental modeling                   |\n",
    "| **Assumptions**       | Weak learners perform slightly better than random guessing              |\n",
    "| **Pros**              | High accuracy, handles mixed data, flexible loss functions              |\n",
    "| **Cons**              | Prone to overfitting, slower to train                                   |\n",
    "| **Key Parameters**    | n_estimators, learning_rate, max_depth, subsample                      |\n",
    "| **Evaluation Metrics**| MSE, RMSE, R² Score                                                     |\n",
    "\n",
    "## Interview Q&A\n",
    "\n",
    "**Q1: What is Gradient Boosting Regression?**  \n",
    "A: An ensemble method that builds models sequentially, each correcting previous errors for regression tasks.\n",
    "\n",
    "**Q2: How does Gradient Boosting Regression work?**  \n",
    "A: It fits new models to the residuals of previous models.\n",
    "\n",
    "**Q3: What are the advantages of Gradient Boosting Regression?**  \n",
    "A: High accuracy, flexible, handles mixed data types.\n",
    "\n",
    "**Q4: What are the limitations?**  \n",
    "A: Prone to overfitting, slower to train.\n",
    "\n",
    "**Q5: How do you prevent overfitting in Gradient Boosting Regression?**  \n",
    "A: Use early stopping, limit tree depth, use subsampling.\n",
    "\n",
    "**Q6: How do you evaluate Gradient Boosting Regression?**  \n",
    "A: Using MSE, RMSE, and R² score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc2f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 2️⃣ Load Dataset\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# 3️⃣ Split Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 4️⃣ Create Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Optional for tree-based models\n",
    "    ('gb', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# 5️⃣ Hyperparameter Tuning\n",
    "param_grid = {\n",
    "    'gb__n_estimators': [100, 200],\n",
    "    'gb__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'gb__max_depth': [3, 4, 5],\n",
    "    'gb__min_samples_split': [2, 5, 10],\n",
    "    'gb__min_samples_leaf': [1, 2, 4],\n",
    "    'gb__max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 6️⃣ Evaluate Best Model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"R2 Score:\", r2_score(y_test, y_pred))\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# 7️⃣ Feature Importance Visualization\n",
    "importances = best_model.named_steps['gb'].feature_importances_\n",
    "feat_imp_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n",
    "feat_imp_df = feat_imp_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feat_imp_df, palette='coolwarm')\n",
    "plt.title(\"Feature Importance in Gradient Boosting Regressor\")\n",
    "plt.show()\n",
    "\n",
    "# 8️⃣ Predicted vs Actual Visualization\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6, color='teal')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Gradient Boosting Regressor: Predicted vs Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef6443e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
